{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Libreria sae_lens**:  link github https://github.com/ale29ruf/sae_lens.git\n",
    "2. **Libreria EleutherAI/delphi**:   link github  https://github.com/godSaveDaniele/progetto_nlp_2.git\n",
    "3. **Libreria EleutherAI/sparsify**: pip install eai-sparsify\n",
    "4. **Libreria loguru**: pip install loguru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installazione librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T16:37:06.341617Z",
     "iopub.status.busy": "2025-10-28T16:37:06.341375Z",
     "iopub.status.idle": "2025-10-28T16:37:08.916025Z",
     "shell.execute_reply": "2025-10-28T16:37:08.915214Z",
     "shell.execute_reply.started": "2025-10-28T16:37:06.341598Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-29T09:38:21.551Z",
     "iopub.execute_input": "2025-10-29T09:37:37.901924Z",
     "iopub.status.busy": "2025-10-29T09:37:37.901667Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/godSaveDaniele/progetto_nlp_2.git\n",
    "%cd /kaggle/working/progetto_nlp_2\n",
    "!pip install -q -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-29T09:38:21.551Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q loguru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametri di configurazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETRI MAX ACTIVATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T07:49:58.816368Z",
     "iopub.status.busy": "2025-10-29T07:49:58.815827Z",
     "iopub.status.idle": "2025-10-29T07:49:58.820238Z",
     "shell.execute_reply": "2025-10-29T07:49:58.819349Z",
     "shell.execute_reply.started": "2025-10-29T07:49:58.816347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name=\"google/gemma-2-2b\"  #nome del modello da spiegare\n",
    "\n",
    "repo_SAE = \"google/gemma-scope-2b-pt-res\"   #repository_SAE\n",
    "hookpoint = \"layer_15/width_65k/average_l0_127\"   #folder_name\n",
    "hookpoint_dir = \"layers.15\" #formato -> layer.numLayer\n",
    "\n",
    "# layer_15/width_16k/average_l0_78\n",
    "# layer_18/width_16k/average_l0_74\n",
    "# layer_21/width_16k/average_l0_70\n",
    "# layer_24/width_16k/average_l0_73\n",
    "\n",
    "num_layer= 15\n",
    "type_layer= \"res\"\n",
    "\n",
    "sample_dim = 40    #numero di latenti da spiegare per ogni layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **dataset_repo** ->  repository del dataset utilizzato per generare le attivazioni dei latenti (feature dell'hidden layer del SAE)\n",
    "\n",
    "2. **n_tokens** -> numero di token su cui andare a valutare i latenti. Più token si hanno a disposizione e più si ha la possibilità di identificare pattern significativi nelle attivazioni del SAE (il modello explainer ha più contesto per identificare pattern significativi).\n",
    "\n",
    "3. **ctx_len** -> lunghezza del contesto fornito in input al modello da spiegare, per generare le attivazioni che verrano poi passate al SAE. Impatta la qualità delle attivazioni del modello da spiegare. Maggiore è il valore di questo parametro e più sono informative le attivazioni.\n",
    "NOTA BENE: Questo è il contesto del modello da spiegare. Il SAE non riceve contesto, ma proietta in uno spazio a dimensione maggiore, l'hidden state relativo ad un solo token per volta. \n",
    "\n",
    "4. **batch_size**  ->  Numero di contesti che vengono processati in parallelo per calcolare le attivazioni. Questo parametro non ha alcuna influenza sulla qualità delle explanations, in quanto i modelli vengono usate in inferenza. Aumentare la batch_size, migliora la velocità, perché permette si sfruttare meglio la GPU, ma può aumentare il consumo di memoria\n",
    "\n",
    "5. **example_ctx_len** -> Lunghezza di un esempio da passare al modello explainer. \n",
    "\n",
    "6. **min_examples** -> Numero minimo di esempi attivanti per poter spiegare un latente. \n",
    "\n",
    "7. **max_examples** -> Numero massimo di esempi per poter spiegare un latente. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T07:50:05.612179Z",
     "iopub.status.busy": "2025-10-29T07:50:05.611711Z",
     "iopub.status.idle": "2025-10-29T07:50:05.616235Z",
     "shell.execute_reply": "2025-10-29T07:50:05.615680Z",
     "shell.execute_reply.started": "2025-10-29T07:50:05.612159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ctx_len = 256\n",
    "seed = 22\n",
    "example_ctx_len = 32\n",
    "max_examples = 10000    \n",
    "min_examples = 200\n",
    "dataset_repo = \"bookcorpus\"\n",
    "dataset_split = \"train[:50%]\"  # frazione di dataset da utilizzare\n",
    "batch_size = 4\n",
    "n_tokens = 3_000_000\n",
    "column_name = \"text\" #colonna del dataset \n",
    "n_non_activating: int = 40 # numero di esempi non attivanti da costruire per latente (50 di default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T11:29:36.064561Z",
     "iopub.status.busy": "2025-10-28T11:29:36.064282Z",
     "iopub.status.idle": "2025-10-28T11:29:37.952721Z",
     "shell.execute_reply": "2025-10-28T11:29:37.951912Z",
     "shell.execute_reply.started": "2025-10-28T11:29:36.064517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T07:50:12.158985Z",
     "iopub.status.busy": "2025-10-29T07:50:12.158331Z",
     "iopub.status.idle": "2025-10-29T07:50:42.507364Z",
     "shell.execute_reply": "2025-10-29T07:50:42.506686Z",
     "shell.execute_reply.started": "2025-10-29T07:50:12.158956Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q --force-reinstall --no-cache-dir \\\n",
    "  \"datasets==3.6.0\" \"huggingface-hub>=0.34,<1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T07:51:28.389846Z",
     "iopub.status.busy": "2025-10-29T07:51:28.389558Z",
     "iopub.status.idle": "2025-10-29T07:51:28.868688Z",
     "shell.execute_reply": "2025-10-29T07:51:28.868165Z",
     "shell.execute_reply.started": "2025-10-29T07:51:28.389826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# Keys and device\n",
    "login(\"\")    #hugging-face-key \n",
    "OPENROUTER_API_KEY = \"\"  #open-router-key\n",
    "GAI_KEY = \"\"     #gemini-key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching delle attivazioni\n",
    "Le attivazioni sono generate attraverso il framework delphi.\n",
    "Il seguente comando si occupa di:\n",
    "- caricare il dataset\n",
    "- tokenizzare il dataset\n",
    "- calcolare per ciascun latente, il valore di attivazione per ogni token di input\n",
    "- fare caching delle attivazioni. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T07:51:33.924625Z",
     "iopub.status.busy": "2025-10-29T07:51:33.924122Z",
     "iopub.status.idle": "2025-10-29T07:51:40.547524Z",
     "shell.execute_reply": "2025-10-29T07:51:40.546622Z",
     "shell.execute_reply.started": "2025-10-29T07:51:33.924602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade --only-binary=:all: \"scikit-learn==1.4.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T07:51:40.549263Z",
     "iopub.status.busy": "2025-10-29T07:51:40.549048Z",
     "iopub.status.idle": "2025-10-29T07:52:08.871149Z",
     "shell.execute_reply": "2025-10-29T07:52:08.870434Z",
     "shell.execute_reply.started": "2025-10-29T07:51:40.549244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q --force-reinstall --no-cache-dir \\\n",
    "  \"numpy==1.26.4\" \"scipy==1.11.4\" \"matplotlib==3.8.4\" \"scikit-learn==1.4.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T07:52:08.872523Z",
     "iopub.status.busy": "2025-10-29T07:52:08.872252Z",
     "iopub.status.idle": "2025-10-29T07:52:12.393689Z",
     "shell.execute_reply": "2025-10-29T07:52:12.392895Z",
     "shell.execute_reply.started": "2025-10-29T07:52:08.872481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade \"transformers[torch]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "execution_failed": "2025-10-29T09:31:13.130Z",
     "iopub.execute_input": "2025-10-29T07:52:12.395468Z",
     "iopub.status.busy": "2025-10-29T07:52:12.395225Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!TRANSFORMERS_NO_TF=1 TRANSFORMERS_NO_JAX=1 python -m delphi \"{model_name}\" {repo_SAE} \\\n",
    "    --dataset_repo \"{dataset_repo}\" \\\n",
    "    --dataset_split \"{dataset_split}\" \\\n",
    "    --batch_size {batch_size} \\\n",
    "    --example_ctx_len {example_ctx_len} \\\n",
    "    --n_tokens {n_tokens} \\\n",
    "    --max_latents 0 \\\n",
    "    --cache_only True \\\n",
    "    --cache_ctx_len {ctx_len} \\\n",
    "    --hookpoints \"{hookpoint}\" \\\n",
    "    --seed {seed} \\\n",
    "    --filter_bos \\\n",
    "    --n_splits 5 \\\n",
    "    --name test-run \\\n",
    "    --num_gpus 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento del modello da spiegare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-29T09:31:13.130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/ale29ruf/sae_lens.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T11:58:27.390685Z",
     "iopub.status.busy": "2025-10-28T11:58:27.390335Z",
     "iopub.status.idle": "2025-10-28T11:58:31.821248Z",
     "shell.execute_reply": "2025-10-28T11:58:31.820295Z",
     "shell.execute_reply.started": "2025-10-28T11:58:27.390662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -q --force-reinstall --no-cache-dir \\\n",
    "  \"pillow>=10.3,<12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T11:58:33.874334Z",
     "iopub.status.busy": "2025-10-28T11:58:33.874021Z",
     "iopub.status.idle": "2025-10-28T11:58:41.120300Z",
     "shell.execute_reply": "2025-10-28T11:58:41.119558Z",
     "shell.execute_reply.started": "2025-10-28T11:58:33.874311Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import re \n",
    "import gc\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import traceback\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import Dict, List, DefaultDict, Tuple\n",
    "from json.decoder import JSONDecodeError\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import functools\n",
    "\n",
    "\n",
    "# Hugging Face and Models\n",
    "from transformers import (\n",
    "    #pipeline,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from huggingface_hub import hf_hub_download, notebook_login, login\n",
    "from openai import OpenAI, RateLimitError\n",
    "import google.generativeai as gai\n",
    "import datasets\n",
    "\n",
    "from torch.nn.functional import cross_entropy\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# SAE and Transformer Lens\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from sae_lens.loading.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from sae_lens.loading.pretrained_sae_loaders import sparsify_huggingface_loader\n",
    "from transformer_lens.utils import test_prompt, tokenize_and_concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T11:58:53.126671Z",
     "iopub.status.busy": "2025-10-28T11:58:53.126367Z",
     "iopub.status.idle": "2025-10-28T11:58:53.130719Z",
     "shell.execute_reply": "2025-10-28T11:58:53.129951Z",
     "shell.execute_reply.started": "2025-10-28T11:58:53.126648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False) # evita la saturazione della memoria nelle fasi di forward dei modelli\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T11:59:44.667769Z",
     "iopub.status.busy": "2025-10-28T11:59:44.667072Z",
     "iopub.status.idle": "2025-10-28T11:59:44.674456Z",
     "shell.execute_reply": "2025-10-28T11:59:44.673778Z",
     "shell.execute_reply.started": "2025-10-28T11:59:44.667740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Model:\n",
    "    model_id: str             \n",
    "    m: HookedSAETransformer = None\n",
    "\n",
    "    \n",
    "    def load_model(self):\n",
    "        model_id = self.model_id\n",
    "        if self.m is None:\n",
    "            self.m = HookedSAETransformer.from_pretrained(self.model_id, device=device, torch_dtype = torch.bfloat16)\n",
    "            \n",
    "\n",
    "    # restituisce il sottoinsieme \"train\" del dataset \"NeelNanda/pile-10k\" in sequenze di token da 32, escludendo gli ultimi 1000 esempi\n",
    "    def get_pile_dataset(self):\n",
    "        dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\")\n",
    "        pile = tokenize_and_concatenate(dataset, self.m.tokenizer, streaming=False, max_length=32, column_name=\"text\", add_bos_token=True, num_proc=4)\n",
    "        # parameter \"streaming\" in \"tokenize_and_concatenate\": Whether the dataset is being streamed. If True, avoids using parallelism. Defaults to False.\n",
    "        pile = pile[:-1000][\"tokens\"]\n",
    "        return pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T11:59:48.474708Z",
     "iopub.status.busy": "2025-10-28T11:59:48.474328Z",
     "iopub.status.idle": "2025-10-28T12:01:02.866289Z",
     "shell.execute_reply": "2025-10-28T12:01:02.865484Z",
     "shell.execute_reply.started": "2025-10-28T11:59:48.474680Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "m = Model(model_name)\n",
    "m.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:01:05.690748Z",
     "iopub.status.busy": "2025-10-28T12:01:05.690435Z",
     "iopub.status.idle": "2025-10-28T12:01:05.700648Z",
     "shell.execute_reply": "2025-10-28T12:01:05.700067Z",
     "shell.execute_reply.started": "2025-10-28T12:01:05.690726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Activation:\n",
    "    id: str\n",
    "    token_values: List[Tuple[str, float]]\n",
    "    max_value: float\n",
    "    min_value: float\n",
    "    max_value_token_index: int = None\n",
    "    normalized_activations: List[int] = None\n",
    "    \n",
    "    def get_tokens(self) -> List[str]:\n",
    "        tokens = [tv[0] for tv in self.token_values]\n",
    "        return tokens\n",
    "\n",
    "    def get_values(self) -> List[float]:\n",
    "        values = [tv[1] for tv in self.token_values]\n",
    "        return values\n",
    "    \n",
    "    def get_tokens_str(self) -> str:\n",
    "        return \"\".join(self.get_tokens())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Tokens={self.get_tokens()}\"\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(str(self))\n",
    "\n",
    "@dataclass\n",
    "class Feature:\n",
    "    model_id: str      # e.g. gemma-2-2b\n",
    "    feature: int       # e.g. 1846\n",
    "    layer: str         # e.g. 11\n",
    "    type: str          # e.g. att, mlp, res\n",
    "    activations: List[Activation] = None\n",
    "    \n",
    "    sae_id: str = None       # from saes_df, e.g. layer_11/width_16k/average_l0_80\n",
    "    sae_release: str = None  # from saes_df, e.g. gemma-scope-2b-pt-mlp\t  \n",
    "    size: str = \"\"         # e.g. 16k, 65k\n",
    "    \n",
    "    def get_size_int(self):\n",
    "        if self.size != \"\":\n",
    "            return size_to_int(self.size)\n",
    "        return 0\n",
    "\n",
    "    def get_max_activating_examples(self, k: int = 10) -> List[Activation]:\n",
    "        return self.activations[:k] # gli esempi attivanti sono già ordinati in modo decrescente\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Feature {self.type}-{self.size}/{self.layer}/{self.feature}\"\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(str(self))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento dello sparse autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sae-lens mette a disposizione un loader specifico per i sae addestrati tramite sparsify: sparsify_huggingface_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Uso il loader Sparsify\n",
    "#cfg_dict, state_dict_loaded, log_sparsity = sparsify_huggingface_loader(\n",
    "#    repo_id=repo_SAE,\n",
    "#    folder_name=hookpoint,\n",
    "#    device='cpu',\n",
    "#    force_download=False,\n",
    "#    cfg_overrides=None\n",
    "#)\n",
    "\n",
    "# print(f\"Config: {cfg_dict}\")\n",
    "# print(f\"State dict keys: {state_dict_loaded.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Correggo le dimensioni dei tensori\n",
    "#state_dict = {}\n",
    "\n",
    "#W_enc = state_dict_loaded[\"W_enc\"]\n",
    "#state_dict[\"W_enc\"] = W_enc\n",
    "\n",
    "#W_dec = state_dict_loaded[\"W_dec\"]\n",
    "#state_dict[\"W_dec\"] = W_dec.T\n",
    "\n",
    "# Bias\n",
    "#state_dict[\"b_enc\"] = state_dict_loaded.get(\"b_enc\", \n",
    "#                                           state_dict_loaded.get(\"encoder.bias\", \n",
    "#                                                                torch.zeros(cfg_dict[\"d_sae\"], device=device)))\n",
    "#state_dict[\"b_dec\"] = state_dict_loaded.get(\"b_dec\", \n",
    "#                                           state_dict_loaded.get(\"decoder.bias\", \n",
    "#                                                                torch.zeros(cfg_dict[\"d_in\"], device=device)))\n",
    "\n",
    "# Step 3: Creo il SAE\n",
    "#sae = SAE.from_dict(cfg_dict)\n",
    "#sae.load_state_dict(state_dict)\n",
    "#sae.cfg.metadata.hook_name=cfg_dict.get('hook_name') # importante per i metodi di hooking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:01:09.936311Z",
     "iopub.status.busy": "2025-10-28T12:01:09.936067Z",
     "iopub.status.idle": "2025-10-28T12:01:12.127991Z",
     "shell.execute_reply": "2025-10-28T12:01:12.127234Z",
     "shell.execute_reply.started": "2025-10-28T12:01:09.936295Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "repo_SAE = repo_SAE[len(\"google/\"):]\n",
    "\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = repo_SAE ,\n",
    "    sae_id = hookpoint,\n",
    "    device=device)\n",
    "\n",
    "sae = sae.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caricamento del modello explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:01:28.854225Z",
     "iopub.status.busy": "2025-10-28T12:01:28.853719Z",
     "iopub.status.idle": "2025-10-28T12:01:28.862893Z",
     "shell.execute_reply": "2025-10-28T12:01:28.862088Z",
     "shell.execute_reply.started": "2025-10-28T12:01:28.854196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Explainer:\n",
    "    def __init__(self, remote: bool, openrouter: bool, gemini=False, default_openrouter_model = \"meta-llama/llama-3.3-70b-instruct\", default_remote_model=\"gpt-4o\", weak_remote_model=\"gpt-4o-mini\", default_local_model=\"meta-llama/Meta-Llama-3-70B-Instruct\"):\n",
    "        self.remote = remote\n",
    "        self.weak_remote_model = weak_remote_model\n",
    "        self.gemini = gemini\n",
    "        self.openrouter = openrouter\n",
    "        \n",
    "        if remote:\n",
    "            \n",
    "            if openrouter:\n",
    "                \n",
    "                self.openrouter_client = OpenAI(\n",
    "                    base_url=\"https://openrouter.ai/api/v1\",\n",
    "                    api_key=OPENROUTER_API_KEY\n",
    "                )\n",
    "                \n",
    "                self.remote_model = default_openrouter_model \n",
    "                \n",
    "            elif gemini:\n",
    "                gai.configure(api_key=GAI_KEY)\n",
    "                self.remote_model = gai.GenerativeModel(\"models/gemini-1.5-pro-latest\")\n",
    "\n",
    "            else:\n",
    "                self.client = OpenAI()\n",
    "                self.remote_model = default_remote_model\n",
    "                \n",
    "        else:\n",
    "            self.local_model = pipeline(\"text-generation\", model=default_local_model, device_map=\"auto\", max_length=10000)\n",
    "            self.local_model.tokenizer.pad_token_id = self.local_model.tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "    def __call__(self, prompts: List[Dict], weak=False):\n",
    "        if self.remote:\n",
    "            if self.openrouter:\n",
    "                try:\n",
    "                    completion = self.openrouter_client.chat.completions.create(\n",
    "                        model=self.remote_model,\n",
    "                        messages=prompts,\n",
    "                        max_tokens=10000,  # equivalente al max_length precedente\n",
    "                        temperature=0.7    # opzionale (controlla la creatività delle risposte)\n",
    "                    )\n",
    "                    return completion.choices[0].message.content\n",
    "                except Exception as e:\n",
    "                    print(f\"Errore nella chiamata a OpenRouter: {e}\")\n",
    "                    return \"Errore nella generazione del testo\"\n",
    "                \n",
    "            elif self.gemini:\n",
    "                prompt = \"\"\n",
    "                for p in prompts:\n",
    "                    prompt += p[\"content\"] + \"\\n\"\n",
    "\n",
    "                return self.remote_model.generate_content(prompt).text\n",
    "\n",
    "            else:\n",
    "                assert weak, \"Only weak mode is supported for OpenAI API\"\n",
    "                remote_model = self.remote_model if not weak else self.weak_remote_model\n",
    "                completion = self.client.chat.completions.create(model=remote_model, messages=prompts)\n",
    "                return completion.choices[0].message.content\n",
    "        \n",
    "        else:\n",
    "            # ATTENZIONE: IN BASE AL MODELLO SCELTO CAMBIA LA STRUTTURA DELL'OUTPUT\n",
    "            return self.local_model(prompts)[0][\"generated_text\"][-1][\"content\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T12:01:30.095087Z",
     "iopub.status.busy": "2025-10-28T12:01:30.094823Z",
     "iopub.status.idle": "2025-10-28T12:01:30.158503Z",
     "shell.execute_reply": "2025-10-28T12:01:30.157762Z",
     "shell.execute_reply.started": "2025-10-28T12:01:30.095069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Inizializzo l'explainer\n",
    "explainer = Explainer(remote = True, openrouter = True, default_local_model = \"meta-llama/Llama-3.1-8B-Instruct\") # remote=False scarica il modello in locale da hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:02:30.398636Z",
     "iopub.status.busy": "2025-10-28T10:02:30.397955Z",
     "iopub.status.idle": "2025-10-28T10:02:30.403392Z",
     "shell.execute_reply": "2025-10-28T10:02:30.402542Z",
     "shell.execute_reply.started": "2025-10-28T10:02:30.398611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Metodo per interrogare l'explainer\n",
    "def get_description(sys_prompt, user_prompt, weak=True) -> str:\n",
    "    score = False\n",
    "    explanation = explainer([\n",
    "        {\"role\": \"system\", \"content\": sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ], weak=weak)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        json_content = explanation.strip(\"json\").strip('`').removeprefix('json\\n').removesuffix('\\n')\n",
    "        j = json.loads(json_content)\n",
    "        explanation = j[\"Explanation\"]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - MaxAct (MA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tecnica **Max Activation** è una tecnica che consente di interpretare il comportamento di una feature di un SAE, sulla base  degli  **input che attivano maggiormente quella feature**, e poi cercano di estrarre patter comuni alle doverse feature. \n",
    "\n",
    "Gli step generali sono: \n",
    "\n",
    "1. **Raccolta delle attivazioni**  \n",
    "   - Si fa passare un ampio insieme di dati attraverso il modello.\n",
    "   - Per ogni feature si registrano i valori di attivazione per ogni esempio. \n",
    "\n",
    "2. **Generazione di interpretazioni**  \n",
    "   - Gli esempi selezionati vengono presentati a un modello explainer che ne ricava una **descrizione** sintetica di ciò che la feature rappresenta.\n",
    "\n",
    "Uno dei limiti fondamentali è legato alla presenza di dead features. Si tratta di features che non si attivano mai (formalmente una feature non viene interpretata, quando il numero di esempi per cui si attiva è inferiore a min_examples, che do default è settato a 200). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:02:33.935789Z",
     "iopub.status.busy": "2025-10-28T10:02:33.935024Z",
     "iopub.status.idle": "2025-10-28T10:02:34.014540Z",
     "shell.execute_reply": "2025-10-28T10:02:34.013796Z",
     "shell.execute_reply.started": "2025-10-28T10:02:33.935766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Optional\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
    "from datasets import load_dataset\n",
    "from sparsify.data import chunk_and_tokenize\n",
    "from safetensors.numpy import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:02:35.969476Z",
     "iopub.status.busy": "2025-10-28T10:02:35.969170Z",
     "iopub.status.idle": "2025-10-28T10:02:35.974216Z",
     "shell.execute_reply": "2025-10-28T10:02:35.973348Z",
     "shell.execute_reply.started": "2025-10-28T10:02:35.969458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def inizialize_environment(start: int, end: int, raw_dir: str = f\"/kaggle/working/progetto_nlp_2/results/test-run/latents/{hookpoint}\"): # directory contenente i dati grezzi dei latenti\n",
    "    # Leggo le attivazioni dai file .safetensor\n",
    "\n",
    "    files_list = os.listdir(raw_dir) # le attivazioni sono sparse su più file\n",
    "    \n",
    "    hookpoints = [cfg_dict['metadata']['hook_name']] # lista contenente i nomi degli hookpoint da analizzare \n",
    "    \n",
    "    latent_range = torch.arange(start, end+1)\n",
    "    \n",
    "    # range di latenti da spiegare\n",
    "    latents = {\n",
    "        hook: latent_range for hook in hookpoints\n",
    "    } \n",
    "    return hookpoints, latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:02:37.239924Z",
     "iopub.status.busy": "2025-10-28T10:02:37.239668Z",
     "iopub.status.idle": "2025-10-28T10:02:37.252211Z",
     "shell.execute_reply": "2025-10-28T10:02:37.251360Z",
     "shell.execute_reply.started": "2025-10-28T10:02:37.239908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Definisco le classi utili per il caricamento dei latenti dai file .safetensor prodotti dal framework delphi\n",
    "class ActivationData(NamedTuple):\n",
    "    \"\"\"\n",
    "    Represents the activation data for a latent.\n",
    "    \"\"\"\n",
    "\n",
    "    locations: Float[Tensor, \"n_examples 2\"]\n",
    "    \"\"\"Tensor of latent locations.\"\"\"\n",
    "\n",
    "    activations: Float[Tensor, \"n_examples\"]\n",
    "    \"\"\"Tensor of latent activations.\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class Latent:\n",
    "    \"\"\"\n",
    "    A latent extracted from a model's activations.\n",
    "    \"\"\"\n",
    "\n",
    "    module_name: str\n",
    "    \"\"\"The module name associated with the latent.\"\"\"\n",
    "\n",
    "    latent_index: int\n",
    "    \"\"\"The index of the latent within the module.\"\"\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        Return a string representation of the latent.\n",
    "\n",
    "        Returns:\n",
    "            str: A string representation of the latent.\n",
    "        \"\"\"\n",
    "        return f\"{self.module_name}_latent{self.latent_index}\"\n",
    "\n",
    "class LatentData(NamedTuple):\n",
    "    \"\"\"\n",
    "    Represents the output of a TensorBuffer.\n",
    "    \"\"\"\n",
    "\n",
    "    latent: Latent\n",
    "    \"\"\"The latent associated with this output.\"\"\"\n",
    "\n",
    "    module: str\n",
    "    \"\"\"The module associated with this output.\"\"\"\n",
    "\n",
    "    activation_data: ActivationData\n",
    "    \"\"\"The activation data for this latent.\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class TensorBuffer:\n",
    "    \"\"\"\n",
    "    Lazy loading buffer for cached splits.\n",
    "    \"\"\"\n",
    "\n",
    "    path: str\n",
    "    \"\"\"Path to the tensor file.\"\"\"\n",
    "\n",
    "    module_path: str\n",
    "    \"\"\"Path of the module.\"\"\"\n",
    "\n",
    "    latents: Optional[Float[Tensor, \"num_latents\"]] = None\n",
    "    \"\"\"Tensor of latent indices.\"\"\"\n",
    "\n",
    "    tokens: Optional[Float[Tensor, \"batch seq\"]] = None\n",
    "    \"\"\"Tensor of tokens.\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterate over the buffer, yielding BufferOutput objects.\n",
    "\n",
    "        Yields:\n",
    "            Union[BufferOutput, None]: BufferOutput if enough examples,\n",
    "                None otherwise.\n",
    "        \"\"\"\n",
    "        latents, split_locations, split_activations = self.load_data_per_latent()\n",
    "\n",
    "        for i in range(len(latents)):\n",
    "            latent_locations = split_locations[i]\n",
    "            latent_activations = split_activations[i]\n",
    "            yield LatentData(\n",
    "                Latent(self.module_path, int(latents[i].item())),\n",
    "                self.module_path,\n",
    "                ActivationData(latent_locations, latent_activations),\n",
    "            )\n",
    "            \n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "    ) -> tuple[\n",
    "        Float[Tensor, \"locations 2\"],\n",
    "        Float[Tensor, \"activations\"],\n",
    "        Float[Tensor, \"batch seq\"] | None,\n",
    "    ]:\n",
    "        \"\"\"Load the tensor buffer's data.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor, Optional[Tensor]]: Locations, activations,\n",
    "                and tokens (if present in the cache).\n",
    "        \"\"\"\n",
    "        split_data = load_file(self.path)\n",
    "        first_latent = int(self.path.split(\"/\")[-1].split(\"_\")[0])\n",
    "        activations = torch.tensor(split_data[\"activations\"])\n",
    "        locations = torch.tensor(split_data[\"locations\"].astype(np.int64))\n",
    "        if \"tokens\" in split_data:\n",
    "            tokens = torch.tensor(split_data[\"tokens\"].astype(np.int64))\n",
    "        else:\n",
    "            tokens = None\n",
    "\n",
    "        locations[:, 2] = locations[:, 2] + first_latent\n",
    "\n",
    "        if self.latents is not None:\n",
    "            wanted_locations = torch.isin(locations[:, 2], self.latents)\n",
    "            locations = locations[wanted_locations]\n",
    "            activations = activations[wanted_locations]\n",
    "\n",
    "        return locations, activations, tokens\n",
    "\n",
    "    def load_data_per_latent(self):\n",
    "        locations, activations, _ = self.load()\n",
    "        indices = torch.argsort(locations[:, 2], stable=True)\n",
    "        activations = activations[indices]\n",
    "        locations = locations[indices]\n",
    "        unique_latents, counts = torch.unique_consecutive(\n",
    "            locations[:, 2], return_counts=True\n",
    "        )\n",
    "        latents = unique_latents\n",
    "        split_locations = torch.split(locations, counts.tolist())\n",
    "        split_activations = torch.split(activations, counts.tolist())\n",
    "\n",
    "        return latents, split_locations, split_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:02:40.501564Z",
     "iopub.status.busy": "2025-10-28T10:02:40.501081Z",
     "iopub.status.idle": "2025-10-28T10:02:40.506048Z",
     "shell.execute_reply": "2025-10-28T10:02:40.505362Z",
     "shell.execute_reply.started": "2025-10-28T10:02:40.501544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Il seguente metodo calcola gli estremi dei latenti basandosi sui file safetensors presenti nella directory\n",
    "# Il nome di un file safetensor infatti, contiene il range di latenti che memorizza al suo interno\n",
    "def calcolate_edges(raw_dir: str) -> list[tuple[int, int]]:\n",
    "    module_dir = Path(raw_dir)\n",
    "    safetensor_files = [f for f in module_dir.glob(\"*.safetensors\")]\n",
    "    edges = []\n",
    "    for file in safetensor_files:\n",
    "        start, end = file.stem.split(\"_\")\n",
    "        edges.append((int(start), int(end)))\n",
    "    edges.sort(key=lambda x: x[0])\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:02:41.885149Z",
     "iopub.status.busy": "2025-10-28T10:02:41.884684Z",
     "iopub.status.idle": "2025-10-28T10:02:42.956632Z",
     "shell.execute_reply": "2025-10-28T10:02:42.955992Z",
     "shell.execute_reply.started": "2025-10-28T10:02:41.885127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Carico il dataset contenente i token su cui i latenti si attivano\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:02:44.636358Z",
     "iopub.status.busy": "2025-10-28T10:02:44.636083Z",
     "iopub.status.idle": "2025-10-28T10:02:44.642951Z",
     "shell.execute_reply": "2025-10-28T10:02:44.642189Z",
     "shell.execute_reply.started": "2025-10-28T10:02:44.636341Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_buffer(hookpoints, latents, raw_dir):\n",
    "    buffers = [] # \"list[TensorBuffer]\" buffer in cui accumuleremo i dati grezzi dei latenti\n",
    "    \n",
    "    for hook in hookpoints:\n",
    "        selected_latents = latents[hook]\n",
    "        edges = calcolate_edges(raw_dir)\n",
    "        if len(selected_latents) == 0:\n",
    "            continue\n",
    "        if len(edges) == 0:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Could not find any safetensor files in {raw_dir}/{hook}, \"\n",
    "                \"but latents were selected.\"\n",
    "            )\n",
    "    \n",
    "        # creo una lista di punti di separazione tra i bucket\n",
    "        boundaries = [edges[0][0]] + [edge[1] + 1 for edge in edges] \n",
    "    \n",
    "        # assegno ogni latent a un bucket basandosi sui boundaries (right=True significa che i valori sui boundaries appartengono al bucket di sinistra)\n",
    "        bucketized = torch.bucketize(selected_latents, torch.tensor(boundaries), right=True)\n",
    "        unique_buckets = torch.unique(bucketized)\n",
    "    \n",
    "        # itero sui bucket unici\n",
    "        for bucket in unique_buckets:\n",
    "    \n",
    "            # per ogni bucket con dati, crea una maschera per filtrare i latenti che gli appartengono\n",
    "            mask = bucketized == bucket \n",
    "            _selected_latents = selected_latents[mask] \n",
    "    \n",
    "            # calcolo gli estremi del file .safetensor di interesse per il bucket corrente\n",
    "            start, end = boundaries[bucket.item() - 1], boundaries[bucket.item()]\n",
    "       \n",
    "            # Adjust end by one as the path avoids overlap\n",
    "            path = f\"{raw_dir}/{start}_{end-1}.safetensors\"\n",
    "            tensor_buffer = TensorBuffer(\n",
    "                path,\n",
    "                hook,\n",
    "                _selected_latents,\n",
    "            )\n",
    "            buffers.append(tensor_buffer)\n",
    "    return buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:02:45.690508Z",
     "iopub.status.busy": "2025-10-28T10:02:45.690193Z",
     "iopub.status.idle": "2025-10-28T10:02:45.699934Z",
     "shell.execute_reply": "2025-10-28T10:02:45.699324Z",
     "shell.execute_reply.started": "2025-10-28T10:02:45.690489Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Definisco ulteriore classi necessarie per l'incapsulamento dei dati dai file .safetensor\n",
    "\n",
    "@dataclass\n",
    "class Example:\n",
    "    \"\"\"\n",
    "    A single example of latent data.\n",
    "    \"\"\"\n",
    "\n",
    "    tokens: Float[Tensor, \"ctx_len\"]\n",
    "    \"\"\"Tokenized input sequence.\"\"\"\n",
    "\n",
    "    activations: Float[Tensor, \"ctx_len\"]\n",
    "    \"\"\"Activation values for the input sequence.\"\"\"\n",
    "\n",
    "    str_tokens: list[str]\n",
    "    \"\"\"Tokenized input sequence as strings.\"\"\"\n",
    "\n",
    "    normalized_activations: Optional[Float[Tensor, \"ctx_len\"]] = None\n",
    "    \"\"\"Activations quantized to integers in [0, 10].\"\"\"\n",
    "\n",
    "    @property\n",
    "    def max_activation(self) -> float:\n",
    "        \"\"\"\n",
    "        Get the maximum activation value.\n",
    "\n",
    "        Returns:\n",
    "            float: The maximum activation value.\n",
    "        \"\"\"\n",
    "        return float(self.activations.max())\n",
    "\n",
    "@dataclass\n",
    "class ActivatingExample(Example):\n",
    "    \"\"\"\n",
    "    An example of a latent that activates a model.\n",
    "    \"\"\"\n",
    "\n",
    "    quantile: int = 0\n",
    "    \"\"\"The quantile of the activating example.\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class NonActivatingExample(Example):\n",
    "    \"\"\"\n",
    "    An example of a latent that does not activate a model.\n",
    "    \"\"\"\n",
    "\n",
    "    distance: float = 0.0\n",
    "    \"\"\"\n",
    "    The distance from the neighbouring latent.\n",
    "    Defaults to -1.0 if not using neighbours.\n",
    "    \"\"\"\n",
    "\n",
    "@dataclass\n",
    "class LatentRecord:\n",
    "    \"\"\"\n",
    "    A record of latent data.\n",
    "    \"\"\"\n",
    "\n",
    "    latent: Latent\n",
    "    \"\"\"The latent associated with the record.\"\"\"\n",
    "\n",
    "    examples: list[ActivatingExample] = field(default_factory=list)\n",
    "    \"\"\"Example sequences where the latent activations, assumed to be sorted in\n",
    "    descending order by max activation.\"\"\"\n",
    "\n",
    "    not_active: list[NonActivatingExample] = field(default_factory=list)\n",
    "    \"\"\"Non-activating examples.\"\"\"\n",
    "\n",
    "    train: list[ActivatingExample] = field(default_factory=list)\n",
    "    \"\"\"Training examples.\"\"\"\n",
    "\n",
    "    test: list[ActivatingExample] | list[list[Example]] = field(default_factory=list)\n",
    "    \"\"\"Test examples.\"\"\"\n",
    "\n",
    "    explanation: str = \"\"\n",
    "    \"\"\"Explanation of the latent.\"\"\"\n",
    "\n",
    "    extra_examples: Optional[list[Example]] = None\n",
    "    \"\"\"Extra examples to include in the record.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def max_activation(self) -> float:\n",
    "        \"\"\"\n",
    "        Get the maximum activation value for the latent.\n",
    "\n",
    "        Returns:\n",
    "            float: The maximum activation value.\n",
    "        \"\"\"\n",
    "        return self.examples[0].max_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:02:49.301961Z",
     "iopub.status.busy": "2025-10-28T10:02:49.301205Z",
     "iopub.status.idle": "2025-10-28T10:02:49.306248Z",
     "shell.execute_reply": "2025-10-28T10:02:49.305391Z",
     "shell.execute_reply.started": "2025-10-28T10:02:49.301938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Funzione di riassegnamento\n",
    "def upload_dataset():\n",
    "    tokens_ds = load_from_disk(\"/kaggle/working/progetto_nlp_2/dataset_tokenized\")\n",
    "    tokens = tokens_ds[\"input_ids\"]\n",
    "    #data = load_dataset(\"bookcorpus/bookcorpus\", streaming=False, trust_remote_code=True)\n",
    "    #data = data.shuffle(seed)\n",
    "    #tokens_ds = chunk_and_tokenize(\n",
    "    #    data,  # type: ignore\n",
    "    #    tokenizer,\n",
    "    #    max_seq_len=ctx_len,\n",
    "    #    text_key='text',\n",
    "    #)\n",
    "    #tokens = tokens_ds['train'][\"input_ids\"]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:02:50.676078Z",
     "iopub.status.busy": "2025-10-28T10:02:50.675797Z",
     "iopub.status.idle": "2025-10-28T10:02:50.695544Z",
     "shell.execute_reply": "2025-10-28T10:02:50.694503Z",
     "shell.execute_reply.started": "2025-10-28T10:02:50.676058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Definisco metodi di utilità per il caricamento dei latenti\n",
    "\n",
    "def _top_k_pools(\n",
    "    max_buffer: Float[Tensor, \"batch\"],\n",
    "    split_activations: Float[Tensor, \"activations ctx_len\"],\n",
    "    buffer_tokens: Float[Tensor, \"batch ctx_len\"],\n",
    "    max_examples: int,\n",
    ") -> tuple[Float[Tensor, \"examples ctx_len\"], Float[Tensor, \"examples ctx_len\"]]:\n",
    "    \"\"\"\n",
    "    Get the top k activation pools.\n",
    "\n",
    "    Args:\n",
    "        max_buffer: The maximum buffer values.\n",
    "        split_activations: The split activations.\n",
    "        buffer_tokens: The buffer tokens.\n",
    "        max_examples: The maximum number of examples.\n",
    "\n",
    "    Returns:\n",
    "        The token windows and activation windows.\n",
    "    \"\"\"\n",
    "    k = min(max_examples, len(max_buffer))\n",
    "    top_values, top_indices = torch.topk(max_buffer, k, sorted=True) # indici degli esempi con attivazione maggiore\n",
    "\n",
    "    activation_windows = torch.stack([split_activations[i] for i in top_indices])\n",
    "    token_windows = buffer_tokens[top_indices]\n",
    "\n",
    "    return token_windows, activation_windows\n",
    "\n",
    "def pool_max_activation_windows(\n",
    "    activations: Float[Tensor, \"examples\"],\n",
    "    tokens: Float[Tensor, \"windows seq\"],\n",
    "    ctx_indices: Float[Tensor, \"examples\"],\n",
    "    index_within_ctx: Float[Tensor, \"examples\"],\n",
    "    ctx_len: int, # -> sarebbe example_ctx_len\n",
    "    max_examples: int,\n",
    ") -> tuple[Float[Tensor, \"examples ctx_len\"], Float[Tensor, \"examples ctx_len\"]]:\n",
    "    \"\"\"\n",
    "    Pool max activation windows from the buffer output and update the latent record.\n",
    "\n",
    "    Args:\n",
    "        activations : The activations.\n",
    "        tokens : The input tokens.\n",
    "        ctx_indices : The context indices.\n",
    "        index_within_ctx : The index within the context.\n",
    "        ctx_len : The context length.\n",
    "        max_examples : The maximum number of examples.\n",
    "    \"\"\"\n",
    "    # unique_ctx_indices: array of distinct context window indices in order of first\n",
    "    # appearance. sequential integers from 0 to batch_size * cache_token_length//ctx_len\n",
    "    # inverses: maps each activation back to its index in unique_ctx_indices\n",
    "    # (can be used to dereference the context window idx of each activation)\n",
    "    # lengths: the number of activations per unique context window index\n",
    "    unique_ctx_indices, inverses, lengths = torch.unique_consecutive(\n",
    "        ctx_indices, return_counts=True, return_inverse=True\n",
    "    ) \n",
    "    # ctx_indices è un tensore 1D che indica in quale example cade ogni attivazione\n",
    "    # con torch.unique_consecutive si ottengono gli indici degli example unici\n",
    "    # lengths è il numero di example unici\n",
    "\n",
    "    # Get the max activation magnitude within each context window\n",
    "    max_buffer = torch.segment_reduce(activations, \"max\", lengths=lengths)\n",
    "\n",
    "    # Deduplicate the context windows\n",
    "    new_tensor = torch.zeros(len(unique_ctx_indices), ctx_len, dtype=activations.dtype)\n",
    "    new_tensor[inverses, index_within_ctx] = activations\n",
    "    # Si crea una matrice con un numero di righe pari al numero di example unici e un numero di colonne pari a example_ctx_len\n",
    "    # Successivamente la matrice viene popolata con le attivazioni latenti, in base all'indice dell'example e all'indice del token all'interno dell'example\n",
    "\n",
    "    tokens = tokens[unique_ctx_indices]\n",
    "\n",
    "    token_windows, activation_windows = _top_k_pools(\n",
    "        max_buffer, new_tensor, tokens, max_examples\n",
    "    )\n",
    "\n",
    "    return token_windows, activation_windows\n",
    "\n",
    "def prepare_non_activating_examples(\n",
    "    tokens: Float[Tensor, \"examples ctx_len\"],\n",
    "    distance: float,\n",
    "    tokenizer: PreTrainedTokenizer ,\n",
    ") -> list[NonActivatingExample]:\n",
    "    \"\"\"\n",
    "    Prepare a list of non-activating examples from input tokens and distance.\n",
    "\n",
    "    Args:\n",
    "        tokens: Tokenized input sequences.\n",
    "        distance: The distance from the neighbouring latent.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        NonActivatingExample(\n",
    "            tokens=toks,\n",
    "            activations=torch.zeros_like(toks),\n",
    "            normalized_activations=None,\n",
    "            distance=distance,\n",
    "            str_tokens=tokenizer.batch_decode(toks),\n",
    "        )\n",
    "        for toks in tokens\n",
    "    ]\n",
    "\n",
    "def random_non_activating_windows(\n",
    "    available_indices: Float[Tensor, \"windows\"],\n",
    "    reshaped_tokens: Float[Tensor, \"windows ctx_len\"],\n",
    "    n_not_active: int,\n",
    "    tokenizer: PreTrainedTokenizer ,\n",
    "    seed: int = 42,\n",
    ") -> list[NonActivatingExample]:\n",
    "    \"\"\"\n",
    "    Generate random non-activating sequence windows and update the latent record.\n",
    "\n",
    "    Args:\n",
    "        record (LatentRecord): The latent record to update.\n",
    "        available_indices (TensorType[\"n_windows\"]): The indices of the windows where\n",
    "        the latent is not active.\n",
    "        reshaped_tokens (TensorType[\"n_windows\", \"ctx_len\"]): The tokens reshaped\n",
    "        to the context length.\n",
    "        n_not_active (int): The number of non activating examples to generate.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    if n_not_active == 0:\n",
    "        return []\n",
    "\n",
    "    # If this happens it means that the latent is active in every window,\n",
    "    # so it is a bad latent\n",
    "    if available_indices.numel() < n_not_active:\n",
    "        print(\"No available randomly sampled non-activating sequences\")\n",
    "        return []\n",
    "    else:\n",
    "        random_indices = torch.randint(\n",
    "            0, available_indices.shape[0], size=(n_not_active,)\n",
    "        )\n",
    "        selected_indices = available_indices[random_indices]\n",
    "\n",
    "    toks = reshaped_tokens[selected_indices]\n",
    "\n",
    "    return prepare_non_activating_examples(\n",
    "        toks,\n",
    "        -1.0,\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "def constructor(\n",
    "    record: LatentRecord,\n",
    "    activation_data: ActivationData, # questo oggetto contiene i dati relativi alle attivazioni del latent\n",
    "    tokens: Float[Tensor, \"batch seq\"],\n",
    "    tokenizer: PreTrainedTokenizer ,\n",
    ") -> LatentRecord | None:\n",
    "    cache_ctx_len = tokens.shape[1]\n",
    "\n",
    "    # Get all positions (posizioni assolute, rispetto al dataset, dei token) where the latent is active\n",
    "    # activation_data.locations[:, 0] -> indice di batch\n",
    "    # activation_data.locations[:, 1] -> indice (relativo) del primo token all'interno della finestra di contesto\n",
    "    # Utile per ricostruire la posizione globale di un token quando i dati sono suddivisi in batch e finestre\n",
    "    flat_indices = (\n",
    "        activation_data.locations[:, 0] * cache_ctx_len\n",
    "        + activation_data.locations[:, 1]\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Nel codice seguente, si identificano le finestre di token di lunghezza example_ctx_len in cui il latente \n",
    "    è attivo, si estraggono le più forti e le si salvano come ActivatingExample.\n",
    "    \"\"\"\n",
    "    \n",
    "    ctx_indices = flat_indices // example_ctx_len # ctx_indices: in quale example cade ciascuna attivazione\n",
    "    index_within_ctx = flat_indices % example_ctx_len # index_within_ctx: posizione all'interno dell'example\n",
    "    reshaped_tokens = tokens.reshape(-1, example_ctx_len) # token suddivisi in example di lunghezza example_ctx_len\n",
    "    n_windows = reshaped_tokens.shape[0] # numero di example totali\n",
    "\n",
    "    unique_batch_pos = ctx_indices.unique() # batch in questo caso indica un example\n",
    "\n",
    "    mask = torch.ones(n_windows, dtype=torch.bool)\n",
    "    mask[unique_batch_pos] = False\n",
    "\n",
    "    # Indices where the latent is not active\n",
    "    non_active_indices = mask.nonzero(as_tuple=False).squeeze() # restituisce gli indici dove la maschera è True, cioè gli indici dei contesti che non attivano il latente\n",
    "\n",
    "    activations = activation_data.activations # estraggo le attivazioni latenti (dei token)\n",
    "    # Add activation examples to the record in place\n",
    "    token_windows, act_windows = pool_max_activation_windows(\n",
    "        activations=activations,\n",
    "        tokens=reshaped_tokens,\n",
    "        ctx_indices=ctx_indices,\n",
    "        index_within_ctx=index_within_ctx,\n",
    "        ctx_len=example_ctx_len,\n",
    "        max_examples=max_examples,\n",
    "    )\n",
    "    # TODO: We might want to do this in the sampler\n",
    "    # we are tokenizing examples that are not going to be used\n",
    "    record.examples = [\n",
    "        ActivatingExample(\n",
    "            tokens=toks,\n",
    "            activations=acts,\n",
    "            normalized_activations=None,\n",
    "            str_tokens=tokenizer.batch_decode(toks),\n",
    "        )\n",
    "        for toks, acts in zip(token_windows, act_windows)\n",
    "    ]\n",
    "\n",
    "    if len(record.examples) < min_examples:\n",
    "        # Not enough examples to explain the latent\n",
    "        return None\n",
    "\n",
    "    non_activating_examples = random_non_activating_windows(\n",
    "        available_indices=non_active_indices,\n",
    "        reshaped_tokens=reshaped_tokens,\n",
    "        n_not_active=n_non_activating,\n",
    "        seed=seed,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    record.not_active = non_activating_examples\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:02:52.269732Z",
     "iopub.status.busy": "2025-10-28T10:02:52.269109Z",
     "iopub.status.idle": "2025-10-28T10:02:52.274585Z",
     "shell.execute_reply": "2025-10-28T10:02:52.273991Z",
     "shell.execute_reply.started": "2025-10-28T10:02:52.269705Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_latent(tokens, latent_data: LatentData, n_examples: int = 40) -> LatentRecord | None: # n_examples è il n. di top esempi attivanti da estrarre per latente\n",
    "    record = LatentRecord(latent_data.latent)\n",
    "    record = constructor(\n",
    "            record=record,\n",
    "            activation_data=latent_data.activation_data,\n",
    "            tokens=tokens,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "    if record is None:\n",
    "        return record\n",
    "\n",
    "    # estraggo i top examples\n",
    "    examples = record.examples\n",
    "    max_activation = record.max_activation\n",
    "    selected_examples = examples[:n_examples]\n",
    "    # normalizzo le attivazioni\n",
    "    eps = 1e-6\n",
    "    max_activation = max(max_activation, eps)\n",
    "    for example in selected_examples:\n",
    "        example.normalized_activations = ((example.activations * 10 / max_activation).floor().clamp(0, 10))\n",
    "\n",
    "    record.train = selected_examples\n",
    "\n",
    "    record.extra_examples = record.not_active\n",
    "    \n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:02:55.070484Z",
     "iopub.status.busy": "2025-10-28T10:02:55.069811Z",
     "iopub.status.idle": "2025-10-28T10:02:55.074478Z",
     "shell.execute_reply": "2025-10-28T10:02:55.073651Z",
     "shell.execute_reply.started": "2025-10-28T10:02:55.070461Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_data(buffers, tokens):\n",
    "    # Estraggo i dati relativi ai latenti dai file .safetensor\n",
    "    records = []\n",
    "    for buffer in buffers:\n",
    "        for data in buffer:\n",
    "            record = process_latent(tokens, data)\n",
    "            if record is not None:\n",
    "                records.append(record)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:02:56.117414Z",
     "iopub.status.busy": "2025-10-28T10:02:56.116874Z",
     "iopub.status.idle": "2025-10-28T10:02:56.122888Z",
     "shell.execute_reply": "2025-10-28T10:02:56.122260Z",
     "shell.execute_reply.started": "2025-10-28T10:02:56.117391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_feature(f: Feature, records):\n",
    "    # Converto le feature del framework delphi\n",
    "    for record in records:\n",
    "        if record.latent.latent_index == f.feature:\n",
    "            for activating_example in record.train:\n",
    "                activation = Activation('',[],0.0,0.0,[])\n",
    "                activation.normalized_activations = activating_example.normalized_activations.int().tolist()\n",
    "                activation.max_value = activating_example.max_activation\n",
    "                activation.min_value = activation.max_value\n",
    "                for idx, (token,value) in enumerate(zip(activating_example.str_tokens,activating_example.activations.float().tolist())):\n",
    "                    activation.token_values.append((token,float(value)))\n",
    "                    if(activation.max_value == float(value)):\n",
    "                        activation.max_value_token_index = idx\n",
    "                    if(float(value) < activation.min_value):\n",
    "                        activation.min_value = float(value)\n",
    "                f.activations.append(activation)\n",
    "    #assert len(f.activations) > 1, f\"No activations for latent {f.feature}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:02:57.387830Z",
     "iopub.status.busy": "2025-10-28T10:02:57.387117Z",
     "iopub.status.idle": "2025-10-28T10:02:57.403852Z",
     "shell.execute_reply": "2025-10-28T10:02:57.403058Z",
     "shell.execute_reply.started": "2025-10-28T10:02:57.387808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Definisco i few shot example per la costruzione del prompt della tecnica MaxAct\n",
    "@dataclass\n",
    "class My_Example:\n",
    "    activation_records: List[Activation]\n",
    "    explanation: str\n",
    "    \n",
    "example1 = My_Example(\n",
    "    activation_records=[\n",
    "        Activation(\n",
    "            id='',\n",
    "            token_values=[\n",
    "                (token, activation)\n",
    "                for token, activation in zip(\n",
    "                    [\n",
    "                        \"t\", \"urt\", \"ur\", \"ro\", \" is\", \" fab\", \"ulously\", \" funny\",\n",
    "                        \" and\", \" over\", \" the\", \" top\", \" as\", \" a\", \" '\", \"very\",\n",
    "                        \" sneaky\", \"'\", \" but\", \"ler\", \" who\", \" excel\", \"s\", \" in\",\n",
    "                        \" the\", \" art\", \" of\", \" impossible\", \" disappearing\", \"/\",\n",
    "                        \"re\", \"app\", \"earing\", \" acts\"\n",
    "                    ],\n",
    "                    [\n",
    "                        -0.71, -1.85, -2.39, -2.58, -1.34, -1.92, -1.69, -0.84,\n",
    "                        -1.25, -1.75, -1.42, -1.47, -1.51, -0.8, -1.89, -1.56,\n",
    "                        -1.63, 0.44, -1.87, -2.55, -2.09, -1.76, -1.33, -0.88,\n",
    "                        -1.63, -2.39, -2.63, -0.99, 2.83, -1.11, -1.19, -1.33,\n",
    "                        4.24, -1.51\n",
    "                    ],\n",
    "                )\n",
    "            ],\n",
    "            max_value=4.24,\n",
    "            min_value=-2.63,\n",
    "            max_value_token_index=32\n",
    "        ),\n",
    "        Activation(\n",
    "            id='',\n",
    "            token_values=[\n",
    "                (token, activation)\n",
    "                for token, activation in zip(\n",
    "                    [\n",
    "                        \"esc\", \"aping\", \" the\", \" studio\", \" ,\", \" pic\", \"col\",\n",
    "                        \"i\", \" is\", \" warm\", \"ly\", \" affecting\", \" and\", \" so\",\n",
    "                        \" is\", \" this\", \" ad\", \"roit\", \"ly\", \" minimalist\", \" movie\",\n",
    "                        \" .\"\n",
    "                    ],\n",
    "                    [\n",
    "                        -0.69, 4.12, 1.83, -2.28, -0.28, -0.79, -2.2, -2.03,\n",
    "                        -1.77, -1.71, -2.44, 1.6, -1, -0.38, -1.93, -2.09,\n",
    "                        -1.63, -1.94, -1.82, -1.64, -1.32, -1.92\n",
    "                    ],\n",
    "                )\n",
    "            ],\n",
    "            max_value=4.12,\n",
    "            min_value=-2.44,\n",
    "            max_value_token_index=1\n",
    "        ),\n",
    "    ],\n",
    "    explanation=\"present tense verbs ending in 'ing'\")\n",
    "    \n",
    "example2 = My_Example(\n",
    "    activation_records=[\n",
    "        Activation(\n",
    "            id='',\n",
    "            token_values=[\n",
    "                (\"as\", -0.14),\n",
    "                (\" sac\", -1.37),\n",
    "                (\"char\", -0.68),\n",
    "                (\"ine\", -2.27),\n",
    "                (\" movies\", -1.46),\n",
    "                (\" go\", -1.11),\n",
    "                (\" ,\", -0.9),\n",
    "                (\" this\", -2.48),\n",
    "                (\" is\", -2.07),\n",
    "                (\" likely\", -3.49),\n",
    "                (\" to\", -2.16),\n",
    "                (\" cause\", -1.79),\n",
    "                (\" massive\", -0.23),\n",
    "                (\" cardiac\", -0.04),\n",
    "                (\" arrest\", 4.46),\n",
    "                (\" if\", -1.02),\n",
    "                (\" taken\", -2.26),\n",
    "                (\" in\", -2.95),\n",
    "                (\" large\", -1.49),\n",
    "                (\" doses\", -1.46),\n",
    "                (\" .\", -0.6),\n",
    "            ],\n",
    "            max_value=4.46,\n",
    "            min_value=-3.49,\n",
    "            max_value_token_index=14,\n",
    "        ),\n",
    "        Activation(\n",
    "            id='',\n",
    "            token_values=[\n",
    "                (\"shot\", -0.09),\n",
    "                (\" perhaps\", -3.53),\n",
    "                (\"'\", -0.72),\n",
    "                (\"art\", -2.36),\n",
    "                (\"istically\", -1.05),\n",
    "                (\"'\", -1.12),\n",
    "                (\" with\", -2.49),\n",
    "                (\"handheld\", -2.14),\n",
    "                (\" cameras\", -1.98),\n",
    "                (\" and\", -1.59),\n",
    "                (\" apparently\", -2.62),\n",
    "                (\" no\", -2),\n",
    "                (\" movie\", -2.73),\n",
    "                (\" lights\", -2.87),\n",
    "                (\" by\", -3.23),\n",
    "                (\" jo\", -1.11),\n",
    "                (\"aquin\", -2.23),\n",
    "                (\" b\", -0.97),\n",
    "                (\"aca\", -2.28),\n",
    "                (\"-\", -2.37),\n",
    "                (\"as\", -1.5),\n",
    "                (\"ay\", -2.81),\n",
    "                (\" ,\", -1.73),\n",
    "                (\" the\", -3.14),\n",
    "                (\" low\", -2.61),\n",
    "                (\"-\", -1.7),\n",
    "                (\"budget\", -3.08),\n",
    "                (\" production\", -4),\n",
    "                (\" swings\", -0.71),\n",
    "                (\" annoy\", -2.48),\n",
    "                (\"ingly\", -1.39),\n",
    "                (\" between\", -1.96),\n",
    "                (\" vert\", -1.09),\n",
    "                (\"igo\", 4.37),\n",
    "                (\" and\", -0.74),\n",
    "                (\" opacity\", -0.5),\n",
    "                (\" .\", -0.62),\n",
    "            ],\n",
    "            max_value=4.37,\n",
    "            min_value=-4,\n",
    "            max_value_token_index=33,\n",
    "        ),\n",
    "    ],\n",
    "    explanation=\"words related to physical medical conditions\",\n",
    ")\n",
    "\n",
    "example3 = My_Example(\n",
    "    activation_records=[\n",
    "        Activation(\n",
    "            id='',\n",
    "            token_values=[\n",
    "                (\"the\", 0),\n",
    "                (\" sense\", 0),\n",
    "                (\" of\", 0),\n",
    "                (\" together\", 1),\n",
    "                (\"ness\", 2),\n",
    "                (\" in\", 0),\n",
    "                (\" our\", 0.23),\n",
    "                (\" town\", 0.5),\n",
    "                (\" is\", 0),\n",
    "                (\" strong\", 0),\n",
    "                (\" .\", 0),\n",
    "            ],\n",
    "            max_value=2,\n",
    "            min_value=0,\n",
    "            max_value_token_index=4,\n",
    "        ),\n",
    "        Activation(\n",
    "            id='',\n",
    "            token_values=[\n",
    "                (\"a\", -0.15),\n",
    "                (\" buoy\", -2.33),\n",
    "                (\"ant\", -1.4),\n",
    "                (\" romantic\", -2.17),\n",
    "                (\" comedy\", -2.53),\n",
    "                (\" about\", -0.85),\n",
    "                (\" friendship\", 0.23),\n",
    "                (\",\", -1.89),\n",
    "                (\" love\", 0.09),\n",
    "                (\",\", -0.47),\n",
    "                (\" and\", -0.5),\n",
    "                (\" the\", -0.58),\n",
    "                (\" truth\", -0.87),\n",
    "                (\" that\", 0.22),\n",
    "                (\" we\", 0.58),\n",
    "                (\"'re\", 1.34),\n",
    "                (\" all\", 0.98),\n",
    "                (\" in\", 2.21),\n",
    "                (\" this\", 2.84),\n",
    "                (\" together\", 1.7),\n",
    "                (\" .\", -0.89),\n",
    "            ],\n",
    "            max_value=2.84,\n",
    "            min_value=-2.53,\n",
    "            max_value_token_index=18,\n",
    "        ),\n",
    "    ],\n",
    "    explanation=\"phrases related to community\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:03:01.033797Z",
     "iopub.status.busy": "2025-10-28T10:03:01.033511Z",
     "iopub.status.idle": "2025-10-28T10:03:01.042954Z",
     "shell.execute_reply": "2025-10-28T10:03:01.042270Z",
     "shell.execute_reply.started": "2025-10-28T10:03:01.033779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def relu(x: float) -> float:\n",
    "    return max(0.0, x)\n",
    "def normalize_activations(activation_record: Activation, activations: List[float], max_activation: float) -> List[int]:\n",
    "    if activation_record.normalized_activations is not None:\n",
    "        return activation_record.normalized_activations # la normalizzazione è la stessa del framework delphi\n",
    "    # Se activation_record.normalized_activation è None, allora è un few shot example\n",
    "    \"\"\"Convert raw neuron activations to integers on the range [0, 10].\"\"\"\n",
    "    if max_activation <= 0:\n",
    "        return [0 for x in activations]\n",
    "    # Relu is used to assume any values less than 0 are indicating the neuron is in the resting\n",
    "    # state. This is a simplifying assumption that works with relu/gelu.\n",
    "    activation_record.normalized_activations = [min(10, math.floor(10 * relu(x) / max_activation)) for x in activations]\n",
    "    return activation_record.normalized_activations\n",
    "\n",
    "def format_activation_record(activation_record: Activation, omit_zeros: bool) -> str:\n",
    "    tokens = activation_record.get_tokens()\n",
    "    normalized_activations = normalize_activations(activation_record, activation_record.get_values(), activation_record.max_value)\n",
    "    if omit_zeros:\n",
    "        tokens = [\n",
    "            token for token, activation in zip(tokens, normalized_activations) if activation > 0\n",
    "        ]\n",
    "        normalized_activations = [x for x in normalized_activations if x > 0]\n",
    "    entries = []\n",
    "    assert len(tokens) == len(normalized_activations)\n",
    "    for token, activation in zip(tokens, normalized_activations):\n",
    "        if token == \"<|begin_of_text|>\" or token == \"<|eot_id|>\":\n",
    "            continue\n",
    "        activation_string = str(int(activation))\n",
    "        entries.append(f\"{token} \\t {activation_string}\")\n",
    "    return \" \\n \".join(entries)\n",
    "\n",
    "def format_activation_records(activation_records: List[Activation], omit_zeros: bool) -> str:\n",
    "    \"\"\"Format a list of activation records into a string.\"\"\"\n",
    "    return (\n",
    "        \" \\n <start> \\n \"\n",
    "        + \" \\n <end> \\n <start> \\n \".join(\n",
    "            [\n",
    "                format_activation_record(activation_record, omit_zeros=omit_zeros)\n",
    "                for activation_record in activation_records\n",
    "            ]\n",
    "        )\n",
    "        + \" \\n <end> \\n \"\n",
    "    )\n",
    "\n",
    "def non_zero_activation_proportion(activation_records: List[Activation]) -> float:\n",
    "    \"\"\"Return the proportion of activation values that aren't zero.\"\"\"\n",
    "    total_activations_count = sum(\n",
    "        [len(activation_record.get_values()) for activation_record in activation_records]\n",
    "    )\n",
    "    normalized_activations = [\n",
    "        activation_record.normalized_activations\n",
    "        for activation_record in activation_records\n",
    "    ]\n",
    "    non_zero_activations_count = sum(\n",
    "        [len([x for x in activations if x != 0]) for activations in normalized_activations]\n",
    "    )\n",
    "    return non_zero_activations_count / total_activations_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:03:03.679320Z",
     "iopub.status.busy": "2025-10-28T10:03:03.679013Z",
     "iopub.status.idle": "2025-10-28T10:03:03.683459Z",
     "shell.execute_reply": "2025-10-28T10:03:03.682804Z",
     "shell.execute_reply.started": "2025-10-28T10:03:03.679297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAX_ACT_DESCRIPTION_PREFIX = \"the main thing this neuron does is find\" # prefisso usato per la spiegazione del latente\n",
    "\n",
    "MAX_ACT_FEW_SHOT_EXAMPLES: List[My_Example] = [example1, example2, example3]\n",
    "\n",
    "MAX_ACT_BASE_SYS_PROMPT = \"We're studying neurons in a neural network. Each neuron looks for some particular \" \\\n",
    "\"thing in a short document. Look at the parts of the document the neuron activates for \" \\\n",
    "\"and summarize in a single sentence what the neuron is looking for. Don't list \" \\\n",
    "\"examples of words.\\n\\nThe activation format is token<tab>activation. Activation \" \\\n",
    "\"values range from 0 to 10. A neuron finding what it's looking for is represented by a \" \\\n",
    "\"non-zero activation value. The higher the activation value, the stronger the match.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:03:04.972826Z",
     "iopub.status.busy": "2025-10-28T10:03:04.972262Z",
     "iopub.status.idle": "2025-10-28T10:03:04.979539Z",
     "shell.execute_reply": "2025-10-28T10:03:04.978632Z",
     "shell.execute_reply.started": "2025-10-28T10:03:04.972804Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_per_neuron_explanation_prompt(\n",
    "    activation_records: List[Activation],\n",
    "    index: int,\n",
    "    repeat_non_zero_activations: bool = True,\n",
    "    numbered_list_of_n_explanations: int = None,\n",
    "    explanation: str = ''):\n",
    "    \n",
    "    message = f\"\"\"Neuron {index + 1}\n",
    "    Activations:{format_activation_records(activation_records, omit_zeros=False)}\"\"\"\n",
    "    \n",
    "    # We repeat the non-zero activations only if it was requested and if the proportion of\n",
    "    # non-zero activations isn't too high.\n",
    "    if (repeat_non_zero_activations) and (non_zero_activation_proportion(activation_records) < 0.2):\n",
    "        message += (\n",
    "            f\"\\nSame activations, but with all zeros filtered out:\"\n",
    "            f\"{format_activation_records(activation_records, omit_zeros=True)}\"\n",
    "        )\n",
    "        \n",
    "    # When set, this indicates that the prompt should solicit a numbered list of the given\n",
    "    # number of explanations, rather than a single explanation.\n",
    "    if numbered_list_of_n_explanations is None:\n",
    "        message += f\"\\nExplanation of neuron {index + 1} behavior:\"\n",
    "        message += f\" {MAX_ACT_DESCRIPTION_PREFIX}\"\n",
    "    \n",
    "    if explanation != '':\n",
    "        message += f\" {explanation}.\"\n",
    "        \n",
    "    return message\n",
    "\n",
    "def generate_max_act_user_prompt(f: Feature, activating_examples: List[Activation] = None):\n",
    "    max_activating = f.get_max_activating_examples(5) # if activating_examples is None else activating_examples\n",
    "    user_prompt = add_per_neuron_explanation_prompt(activation_records=max_activating, index=0, repeat_non_zero_activations=False)\n",
    "    return user_prompt\n",
    "\n",
    "def build_sys_prompt():\n",
    "    prompt = MAX_ACT_BASE_SYS_PROMPT\n",
    "    for i, few_shot_example in enumerate(MAX_ACT_FEW_SHOT_EXAMPLES):\n",
    "        prompt += add_per_neuron_explanation_prompt(activation_records=few_shot_example.activation_records, index=i,\n",
    "                                                    explanation=few_shot_example.explanation)\n",
    "        prompt += \"\\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining surprisal score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:03:05.806650Z",
     "iopub.status.busy": "2025-10-28T10:03:05.806054Z",
     "iopub.status.idle": "2025-10-28T10:03:05.812891Z",
     "shell.execute_reply": "2025-10-28T10:03:05.812075Z",
     "shell.execute_reply.started": "2025-10-28T10:03:05.806628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_prompt = (\n",
    "    \"The following is a description of a certain latent of text and a list of examples that contain the latent.\\n\"\n",
    "    \"Description: \\n\"\n",
    "    \"References to the Antichrist, the Apocalypse and conspiracy theories related to those topics. \\n\"\n",
    "    \"Sentences: \\n\"\n",
    "    \" by which he distinguishes Antichrist is, that he would rob God of his honour and take it to himself, he gives the leading latent which we ought \\n\"\n",
    "    \"3 begins. And the rise of Antichrist. Get ready with  \\n\"\n",
    "    \" would be destroyed. The worlds economy would likely collapse as a result and could usher in a one world government movement. I wrote a small 6 page \\n\"\n",
    "    \"Description: \\n\"\n",
    "    \"Sentences containing digits forming a four-digit year, \\n\"\n",
    "    \"Sentences: \\n\"\n",
    "    \" 20, 2013 at 7:41 pm Martin Smith \\n\"\n",
    "    \" of 2012. In other words, Italy's  \\n\"\n",
    "    \"end 2012 levels). In the first quarter of 2013, we expect revenue to be up slightly from the fourth quarter \\n\"\n",
    "    \"Description: \\n\"\n",
    "    \"Text related to banking and financial institutions \\n\"\n",
    "    \"Sentences: \\n\"\n",
    "    \": He is on the Board of Directors with the Lumbee Bank  \\n\"\n",
    "    \" refurbishing the Bank’s branches.\\nBIP reached 400 thousand users in one year\\nThe use of BIP has already doubled\\nThe \\n\"\n",
    "    \" the Federal Deposit Insurance Corp. \\n\"\n",
    "    \"Description: \\n\"\n",
    "    \"Occurrences of the word 'The' at the beginning of sentences\"\n",
    "    \"Sentences: \\n\"\n",
    "    \"The Smoking Tire hits the canyons with one of the fastest Audi's on the road \\n\"\n",
    "    \"The Chairman of the ABI \\n\"\n",
    "    \"The administrative center is the town of Koch. \\n\"\n",
    "    \"Description: \\n\"\n",
    "    \"Educational qualifications and degrees. \\n\"\n",
    "    \"Sentences: \\n\"\n",
    "    \"He also has a Masters in Business Administration degree from the National University. \\n\"\n",
    "    \"Robert J. McDonnell, B.S., Captain, USAF: Investigation of the High Angle of Fall of the Space Shuttle Challenger. \\n\"\n",
    "    \"He earned his PhD from the \\n\"\n",
    "    \"Description: \\n\"\n",
    "    \"Closing parenthesis and new line characters in programming languages.\"\n",
    "    \"Sentences: \\n\"\n",
    "    \":COLOR_BACKGROUND);\\ntft.add_text\\n\"\n",
    "    \"LOGICAL_X_POSITION(current_position.x)));\\ntf\"\n",
    "    \"Description: \\n\"\n",
    "    \"Hours and dates, mostly related to court operations. \\n\"\n",
    "    \"Sentences: \\n\"\n",
    "    \"The staff is available Monday through Friday from 8:30 \\n\"\n",
    "    \"when it was docketed in this court on January 23 \\n\"\n",
    "    \"Simpson mailed his complaint on or soon after January 16 \\n\"\n",
    "    \"Description: \\n\"\n",
    "    \"References to groups or communities, mostly women or men. \\n\"\n",
    "    \"Sentences: \\n\"\n",
    "    \"also retire from the little group of 50 beggar women with whom she had been working \\n\"\n",
    "    \"The men change out of jeans and sneakers \\n\"\n",
    "    \"Description: \\n\"\n",
    "    \"Terms and phrases related to 'rec' or 're' prefixes \\n\"\n",
    "    \"Sentences: \\n\"\n",
    "    \"1928 Recliner A recliner is a reclining chair. \\n\"\n",
    "    \"the memductance which is the reciprocal of memristance \\n\"\n",
    "    \" Recruits for this \\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:03:07.639476Z",
     "iopub.status.busy": "2025-10-28T10:03:07.638493Z",
     "iopub.status.idle": "2025-10-28T10:03:07.645164Z",
     "shell.execute_reply": "2025-10-28T10:03:07.644556Z",
     "shell.execute_reply.started": "2025-10-28T10:03:07.639448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Sequence\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class SurprisalOutput:\n",
    "    text: str\n",
    "    \"\"\"The text that was used to evaluate the surprisal\"\"\"\n",
    "\n",
    "    distance: float | int\n",
    "    \"\"\"Quantile or neighbor distance\"\"\"\n",
    "\n",
    "    no_explanation: list[float] = field(default_factory=list)\n",
    "    \"\"\"What is the surprisal of the model with no explanation\"\"\"\n",
    "\n",
    "    explanation: list[float] = field(default_factory=list)\n",
    "    \"\"\"What is the surprisal of the model with an explanation\"\"\"\n",
    "\n",
    "    activations: list[float] = field(default_factory=list)\n",
    "    \"\"\"What are the activations of the model\"\"\"\n",
    "\n",
    "\n",
    "class Sample(NamedTuple):\n",
    "    text: str\n",
    "    activations: list[float]\n",
    "    data: SurprisalOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:03:09.189060Z",
     "iopub.status.busy": "2025-10-28T10:03:09.188789Z",
     "iopub.status.idle": "2025-10-28T10:03:09.194828Z",
     "shell.execute_reply": "2025-10-28T10:03:09.194046Z",
     "shell.execute_reply.started": "2025-10-28T10:03:09.189040Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def examples_to_samples(\n",
    "    examples: Sequence[Example],\n",
    ") -> list[Sample]:\n",
    "    samples = []\n",
    "    for example in examples:\n",
    "        assert isinstance(example, ActivatingExample) or isinstance(\n",
    "            example, NonActivatingExample\n",
    "        )\n",
    "        assert example.str_tokens is not None\n",
    "        text = \"\".join(str(token) for token in example.str_tokens)\n",
    "        activations = example.activations.tolist()\n",
    "        samples.append(\n",
    "            Sample(\n",
    "                text=text,\n",
    "                activations=activations,\n",
    "                data=SurprisalOutput(\n",
    "                    activations=activations,\n",
    "                    text=text,\n",
    "                    distance=(\n",
    "                        1\n",
    "                        if isinstance(example, ActivatingExample)\n",
    "                        else 0\n",
    "                    ),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        #print(\"Sample's type: \", sample.data.distance )\n",
    "        #print(\"Sample's activations: \", sample.activations)\n",
    "\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:03:10.265437Z",
     "iopub.status.busy": "2025-10-28T10:03:10.265149Z",
     "iopub.status.idle": "2025-10-28T10:03:10.272916Z",
     "shell.execute_reply": "2025-10-28T10:03:10.272129Z",
     "shell.execute_reply.started": "2025-10-28T10:03:10.265417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_loss_with_kv_cache(\n",
    "        explanation: str, samples: list[Sample], batch_size=2\n",
    "    ):\n",
    "        assert tokenizer is not None, \"Tokenizer is not set in model.tokenizer\"\n",
    "        # Tokenize explanation\n",
    "        tokenizer.padding_side = \"right\"\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        explanation_tokens = tokenizer.encode(\n",
    "            explanation, return_tensors=\"pt\", add_special_tokens=False\n",
    "        ).to(device)\n",
    "        # Generate KV cache for explanation\n",
    "        #explanation_tokens = explanation_tokens.repeat_interleave(batch_size, dim=0)\n",
    "\n",
    "        #with torch.inference_mode():\n",
    "        #    outputs = m.m(input_ids=explanation_tokens, use_cache=True)\n",
    "        #    kv_cache = outputs.past_key_values\n",
    "        total_losses = []\n",
    "        for i in range(0, len(samples), batch_size):\n",
    "            batch_samples = samples[i : i + batch_size]\n",
    "            current_batch_size = len(batch_samples)\n",
    "            \n",
    "            #if current_batch_size < batch_size:\n",
    "            #    explanation_tokens = explanation_tokens.repeat_interleave(\n",
    "            #        current_batch_size, dim=0\n",
    "            #    )\n",
    "            #    with torch.inference_mode():\n",
    "            #        outputs = m.m(input_ids=explanation_tokens, use_cache=True)\n",
    "            #        kv_cache = outputs.past_key_values\n",
    "\n",
    "            # Tokenize full input (explanation + prompts)\n",
    "            full_inputs = [explanation+sample.text for sample in batch_samples]\n",
    "            tokenized_inputs = tokenizer(\n",
    "                full_inputs, return_tensors=\"pt\", padding=True, add_special_tokens=False\n",
    "            ).to(device)\n",
    "\n",
    "            # Prepare input for the model (including explanation)\n",
    "            input_ids = tokenized_inputs.input_ids\n",
    "            attention_mask = tokenized_inputs.attention_mask\n",
    "            labels = input_ids.clone()\n",
    "            labels[~attention_mask.bool()] = -100\n",
    "            # Forward pass using KV cache\n",
    "            with torch.inference_mode():\n",
    "                logits = m.m(input_ids\n",
    "                    #input_ids=input_ids,\n",
    "                    # attention_mask=attention_mask,\n",
    "                    #past_key_values=kv_cache,\n",
    "                )\n",
    "            # Compute loss\n",
    "            #logits = outputs.logits # in HookedTransformer il forward restituisce direttamente i logits\n",
    "\n",
    "            for j, logit in enumerate(logits):\n",
    "                loss = cross_entropy(\n",
    "                    logit[:-1], labels[j][1:], reduction=\"none\"\n",
    "                ).tolist()\n",
    "                # Remove the trailing zeros from the loss\n",
    "                #loss = loss[: attention_mask[j].sum().item()]\n",
    "\n",
    "                # mi interessa fare la media delle loss dei vari token nel contesto\n",
    "                loss = loss[: attention_mask[j].sum().item()]\n",
    "                \n",
    "                total_losses.append(sum(loss)/len(loss))\n",
    "        return total_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:03:11.531505Z",
     "iopub.status.busy": "2025-10-28T10:03:11.531204Z",
     "iopub.status.idle": "2025-10-28T10:03:11.537509Z",
     "shell.execute_reply": "2025-10-28T10:03:11.536738Z",
     "shell.execute_reply.started": "2025-10-28T10:03:11.531485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def query(explanation: str, samples: list[Sample]):\n",
    "        explanation_prompt = (\n",
    "            base_prompt + \"Description: \\n\" + explanation + \"\\n Sentences:\\n\"\n",
    "        )\n",
    "        no_explanation_prompt = (\n",
    "            base_prompt\n",
    "            + \"Description: \\n\"\n",
    "            + \"Various unrelated sentences.\"\n",
    "            + \"\\n Sentences:\\n\"\n",
    "        )\n",
    "\n",
    "        no_explanation_losses = compute_loss_with_kv_cache(\n",
    "            no_explanation_prompt, samples, batch_size=1 ###\n",
    "        )\n",
    "        explanation_losses = compute_loss_with_kv_cache(\n",
    "            explanation_prompt, samples, batch_size=1 ###\n",
    "        )\n",
    "        results = []\n",
    "    \n",
    "        info_value = []\n",
    "        true_label = []\n",
    "        for i in range(len(samples)):\n",
    "            samples[i].data.no_explanation = [no_explanation_losses[i]] # no_explanation deve essere una lista\n",
    "            samples[i].data.explanation = [explanation_losses[i]] # explanation deve essere una lista\n",
    "            results.append(samples[i].data)\n",
    "            info_value.append( (-explanation_losses[i]) - (-no_explanation_losses[i]))\n",
    "            true_label.append(samples[i].data.distance)\n",
    "        #print(info_value)\n",
    "        #print(true_label)\n",
    "        return results, info_value, true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:03:12.390002Z",
     "iopub.status.busy": "2025-10-28T10:03:12.389736Z",
     "iopub.status.idle": "2025-10-28T10:03:12.395103Z",
     "shell.execute_reply": "2025-10-28T10:03:12.394271Z",
     "shell.execute_reply.started": "2025-10-28T10:03:12.389982Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calcolate_surprisal_score(explanation, records):\n",
    "    for record in records:\n",
    "        if record.latent.latent_index == f.feature:\n",
    "            assert record.extra_examples is not None, \"No extra examples provided\"\n",
    "            samples = examples_to_samples(\n",
    "                record.extra_examples,\n",
    "            )\n",
    "            \n",
    "            n_available = len(record.train)\n",
    "            n_to_sample = min(n_non_activating, n_available)\n",
    "            print(f\"N. activating example for latent {f.feature}: {n_to_sample}\")\n",
    "            samples.extend(examples_to_samples(\n",
    "                random.sample(record.train, n_to_sample),\n",
    "            ))\n",
    "            \n",
    "            random.shuffle(samples)\n",
    "            \n",
    "            result, info_value, true_label = query(explanation, samples)\n",
    "\n",
    "            y_true = np.array(true_label)  # etichette vere\n",
    "            y_scores = np.array(info_value)  # probabilità predette\n",
    "            auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "            return result, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T10:03:13.031204Z",
     "iopub.status.busy": "2025-10-28T10:03:13.030538Z",
     "iopub.status.idle": "2025-10-28T10:03:13.035862Z",
     "shell.execute_reply": "2025-10-28T10:03:13.035239Z",
     "shell.execute_reply.started": "2025-10-28T10:03:13.031182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_MaxAct_explanation(f: Feature): \n",
    "    raw_dir = f\"/kaggle/working/progetto_nlp_2/results/test-run/latents/{hookpoint_dir}\"\n",
    "    ###raw_dir = \"/kaggle/input/latents-prov/kaggle/working/progetto_nlp_2/results/test-run/latents/layers.24\"\n",
    "    hookpoints, latents = inizialize_environment(f.feature-1, f.feature+2, raw_dir)\n",
    "    buffers = create_buffer(hookpoints, latents, raw_dir)\n",
    "    tokens = upload_dataset()\n",
    "    records = extract_data(buffers, tokens)\n",
    "    convert_feature(f, records)\n",
    "\n",
    "    MAX_ACT_SYS_PROMPT = build_sys_prompt()\n",
    "    max_act_user_prompt = generate_max_act_user_prompt(f, activating_examples=f.activations)\n",
    "    max_act_exp = get_description(MAX_ACT_SYS_PROMPT, max_act_user_prompt)\n",
    "\n",
    "    list_of_SurprisalOutput, surprisal_score = calcolate_surprisal_score(max_act_exp, records)\n",
    "    \n",
    "    return max_act_exp, surprisal_score\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#f = Feature(model_name, 1, num_layer, type_layer, [], size=\"16k\")\n",
    "#max_act_exp, surprisal_score = get_MaxAct_explanation(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - VocabProj (Vocabulary projection)"
   ]
  },
  {
   "attachments": {
    "d0974b85-d26c-47bd-b231-495ef6434366.png": {
     "image/png": "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"
    },
    "d8150ffc-6979-4bc7-a6e8-ff887d528d68.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "\n",
    "VocabProj interpreta una feature,  proiettando il vettore della feature nello spazio del vocabolario per ottenere un vettore di logit interpretabili. \n",
    "\n",
    "Questa operazione è un’applicazione diretta del **logit lens**: lo stesso meccanismo usato per interpretare hidden states viene usato qui per interpretare feature.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Meccanismo generale del Logit Lens\n",
    "\n",
    "Dato un hidden state generico `x` estratto da un layer del modello:\n",
    "1. si applica la **layer norm finale** (`LN(x)`),\n",
    "2. si proietta nello spazio del vocabolario applicando la matrice di **unembedding** `W_U`.\n",
    "\n",
    "Questo produce un vettore di logits `ℓ(x)`, interpretabile come le predizioni di token che il modello farebbe se `x` fosse lo stato attuale.\n",
    "\n",
    "\n",
    "![Screenshot 2025-09-08 alle 21.53.19.png](attachment:d8150ffc-6979-4bc7-a6e8-ff887d528d68.png)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Applicazione dei logit-lens per interpretare un feature\n",
    "\n",
    "Lo stesso approccio può essere applicato per proiettare, non un generico hidden state, ma un vettore corrispondente ad una particolare feature.\n",
    "\n",
    "\n",
    "Il codice gestisce due casi:\n",
    "\n",
    "- **Caso senza SAE**  \n",
    "  La feature corrisponde a un neurone nel MLP.  \n",
    "  - Si prende la **riga di `W_out`** corrispondente, che rappresenta la direzione nel residuo attivata da quel neurone.  \n",
    "  - Si applica `ln_final` e poi `unembed`, ottenendo i logits.\n",
    "\n",
    "- **Caso con SAE**  \n",
    "  La feature corrisponde a un **latente del SAE**.  \n",
    "  - Si prende la colonna `i` della matrice di decoding `W_dec`, che rappresenta il vettore nel residuo ricostruito da quella feature.  \n",
    "  - Si applica `ln_final` e poi `unembed`, ottenendo i logits.\n",
    "\n",
    "![Screenshot 2025-09-09 alle 10.45.18.png](attachment:d0974b85-d26c-47bd-b231-495ef6434366.png)\n",
    "\n",
    "\n",
    "Da questi logits si estraggono:\n",
    "  - i token più **promossi** (`top_tokens`),\n",
    "  - i token più **soppressi** (`bottom_tokens`),\n",
    "\n",
    "Questi token vengono poi passati a un modello explainer per generare una **descrizione interpretabile della feature**, catturandone l’impatto sulle probabilità dei token del modello.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:11:04.370985Z",
     "iopub.status.busy": "2025-10-27T12:11:04.370242Z",
     "iopub.status.idle": "2025-10-27T12:11:04.375353Z",
     "shell.execute_reply": "2025-10-27T12:11:04.374605Z",
     "shell.execute_reply.started": "2025-10-27T12:11:04.370961Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "VOCAB_PROJ_SYS_PROMPT = \"You will be given a list of tokens related to a specific vector. These tokens represent a combination of embeddings that reconstruct the vector. Your task is to infer the most likely meaning or function of the vector based on these tokens. The list may include noise, such as unrelated terms, symbols, or programming jargon. Ignore whether the words are in multiple different languages, and do not mention it in your response. Focus on identifying a cohesive theme or concept shared by the most relevant tokens. Provide a specific sentence summarizing the meaning or function of the vector. Answer only with the summary. Avoid generic or overly broad answers, and disregard any noise in the list.\\nVector 1\\n    Tokens: ['contentLoaded', '▁hObject', ':✨', '▁AssemblyCulture', 'ContentAsync', '▁ivelany', '▁nahilalakip', 'IUrlHelper', '▁تضيفلها', '▁ErrIntOverflow'] ['▁could', 'could', '▁Could', 'Could', '▁COULD', '▁podría', '▁könnte', '▁podrían', '▁poderia', '▁könnten']\\nExplanation of vector 1 behavior: this vector is related to the word could.\\nVector 2\\n    Tokens: ['▁CreateTagHelper', '▁ldc', 'PropertyChanging', '▁jsPsych', 'ulement', '▁IBOutlet', '▁wireType', '▁initComponents', '▁متعلقه', 'Бахар'] ['▁مشين', '▁charity', '▁donation', '▁charitable', '▁volont', '▁donations', 'iNdEx', 'Parcelize', 'DatabaseError', 'BufferException']\\nExplanation of vector 2 behavior: this vector is related to charity and donations.\\nVector 3\\n    Tokens: ['▁tomorrow', '▁tonight', '▁yesterday', '▁today', 'yesterday', 'tomorrow', '▁demain', '▁Tomorrow', 'Tomorrow', '▁Yesterday'] ['▁Wex', 'ကိုးကား', 'Ārējās', 'piecze', ')$/,', '▁außer', '[]=$', 'cendental', 'ɜ', 'aderie']\\nExplanation of vector 3 behavior: this vector is related to specific dates, like tomorrow, tonight and yesterday.\\n\\n\"\n",
    "VOCAB_PROJ_USER_PROMPT = \"Vector 4\\n    Tokens: {0}\\nExplanation of vector 4 behavior: this vector is related to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:11:06.712620Z",
     "iopub.status.busy": "2025-10-27T12:11:06.711905Z",
     "iopub.status.idle": "2025-10-27T12:11:06.720046Z",
     "shell.execute_reply": "2025-10-27T12:11:06.719385Z",
     "shell.execute_reply.started": "2025-10-27T12:11:06.712595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Esistono alcune varianti per la proiezione dei dati:\n",
    "a)  utilizzare la matrice di encoding del SAE, al posto della matrice di decoding (encode=True)\n",
    "b)  utilizzare la matrice di embedding al posto della matrice di unembedding (embed=False)\n",
    "\n",
    "La combinazione che si mostra avere le performance miglior è quella decoding-unembedding (encode= False, embed=False)\n",
    "\"\"\"\n",
    "\n",
    "def get_projection_data(m: Model, f: Feature, sae_w=None, encode=False, embed=False, k=50):\n",
    "    # project properly and get logits\n",
    "    if sae_w is None:\n",
    "        logits = m.m.unembed(m.m.ln_final(m.m.blocks[f.layer].mlp.W_out[f.feature]))\n",
    "    else:\n",
    "        feature_vector = sae_w[:, f.feature] if encode else sae_w[f.feature]\n",
    "        logits = (feature_vector @ m.m.embed.W_E.T) if embed else m.m.unembed(m.m.ln_final(feature_vector))\n",
    "        \n",
    "    topk = logits.topk(k)\n",
    "    bottomk = logits.topk(k, largest=False) # k token più soppressi\n",
    "    abs_topk = logits.abs().topk(k * 2) # k*2 token con variazioni assolute più forti\n",
    "    top_tokens = m.m.to_str_tokens(topk.indices)\n",
    "    bottom_tokens = m.m.to_str_tokens(bottomk.indices)\n",
    "    top_abs_tokens = m.m.to_str_tokens(abs_topk.indices)\n",
    "    return (logits, topk, bottomk, abs_topk), (top_tokens, bottom_tokens, top_abs_tokens)\n",
    "\n",
    "def get_dec_unembed(m: Model, f: Feature, sae=None):\n",
    "    # sometimes these types don't match (e.g. llama's sae_w_dec is 'torch.bfloat16' while the model is 'torch.float32')\n",
    "    sae_w_dec = None\n",
    "    if sae is not None:\n",
    "        sae_w_dec = sae.W_dec\n",
    "        if sae.W_dec.dtype != m.m.W_U.dtype:\n",
    "            sae_w_dec = sae.W_dec.to(m.m.W_U.dtype)\n",
    "    \n",
    "    # the tokens we actually use - 'value' vectors of SAEs projected using the unembedding matrix #\n",
    "    _, tokens = get_projection_data(m, f, sae_w_dec)\n",
    "    (dec_top_tokens, dec_bottom_tokens, dec_top_abs_tokens) = tokens\n",
    "    \n",
    "    # take the dec unembed data\n",
    "    return (dec_top_tokens, dec_bottom_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:11:08.832111Z",
     "iopub.status.busy": "2025-10-27T12:11:08.831413Z",
     "iopub.status.idle": "2025-10-27T12:11:08.835774Z",
     "shell.execute_reply": "2025-10-27T12:11:08.835217Z",
     "shell.execute_reply.started": "2025-10-27T12:11:08.832085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_VocabProj_explanation(f: Feature):\n",
    "    top_tokens, bottom_tokens = get_dec_unembed(m, f, sae=sae)\n",
    "    vocab_proj_user_prompt = VOCAB_PROJ_USER_PROMPT.format(top_tokens + bottom_tokens)\n",
    "\n",
    "    vocab_proj_exp = get_description(VOCAB_PROJ_SYS_PROMPT, vocab_proj_user_prompt)\n",
    "    return vocab_proj_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - TokenChange (TC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il metodo **Token Change** cerca di spiegare una feature sulla base di quei **token la cui probabilità di generazione cambia maggiormente quando si amplifica l’attivazione di quella specifica feature**.\n",
    "\n",
    "\n",
    "1. **Campionamento dei prompt:** si selezionano casualmente `k` prompt da un dataset (es. *The Pile*).  \n",
    "2. **Calcolo logits di riferimento:** si esegue il modello senza interventi per raccogliere i logits puliti.  I logit vengono raccolti per ogni prompt e per ogni token position. \n",
    "4. **Intervento causale:**  \n",
    "   - Si forza la feature `f` ad un determinato valore e si generano nuovamente i logits.\n",
    "   - Nel caso in cui la feature in questione sia un latente del SAE, avviene il processo di steering con SAE per come descritto di seguito.\n",
    "5. **Differenza media:** si calcola la variazione media (sulle diverse token position dei diversi prompt), dei logits steered rispetto ai logit non steered.      \n",
    "6. **Descrizione della feature:**  - Si selezionano i token con le più alte variazioni di probabilità. La lista dei token viene passata a un modello “explainer”, che restituisce una frase riassuntiva sul significato o la funzione della feature.\n",
    "\n",
    "Tipicamente si fa clamping della feature sia ad un valore positivo che ad un valore negativo, in maniera tale da indiduare:\n",
    "- i token con maggiore incremento di logit quando la feature viene amplificata\n",
    "- i token con maggiore diminuzione di logit quando la feature viene soppressa.\n",
    "\n",
    "\n",
    "A differenza di **VocabProj** (che è un metodo puramente correlativo), **Token Change** rappresenta un approccio causale, poiché interviene direttamente nell’attivazione della feature per osservare come questo influenza la generazione del modello.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Steering with Sparse SAE\n",
    "\n",
    "Il **feature steering** è una tecnica che permette di modificare il comportamento di un modello\n",
    "manipolando direttamente le attivazioni di una singola **feature** identificata da un SAE\n",
    "(*Sparse Autoencoder*).\n",
    "\n",
    "### Procedura\n",
    "\n",
    "1. **Decomposizione delle attivazioni**\n",
    "   - Si prende il vettore di attivazioni del modello `x` e lo si scompone in:\n",
    "     - **SAE(x)** → ricostruzione prodotta dal SAE.\n",
    "     - **error(x)** → errore di ricostruzione, cioè `x - SAE(x)`.\n",
    "\n",
    "2. **Modifica di una feature**\n",
    "   - Si calcola la rappresentazione sparsa `z = E(x)`.\n",
    "   - Si **clampa** la feature di interesse `z_i` a un valore scelto `c`,\n",
    "   - Si ricostruisce un nuovo vettore di attivazioni: `x'`\n",
    "     \n",
    "     \n",
    "   - Opzionalmente si aggiunge l’errore di ricostruzione `l = x - x'`\n",
    "     per preservare meglio l’informazione originale\n",
    "     \n",
    "\n",
    "3. **Propagazione in avanti**\n",
    "   - Si usa il vettore di attivazione così ottenuto come input per i layer successivi del modello,\n",
    "     osservando come cambia l’output.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:11:12.015274Z",
     "iopub.status.busy": "2025-10-27T12:11:12.014522Z",
     "iopub.status.idle": "2025-10-27T12:11:12.018629Z",
     "shell.execute_reply": "2025-10-27T12:11:12.017843Z",
     "shell.execute_reply.started": "2025-10-27T12:11:12.015247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TOKEN_CHANGE_SYS_PROMPT = \"You will be given a list of tokens related to a feature in an LLM. These tokens are the ones whose probabilities changed most when amplifying the feature. Your task is to infer the most likely meaning or function of the feature based on these tokens. The list may include noise, such as unrelated terms, symbols, or programming jargon. Provide a specific sentence summarizing the meaning or function of the feature. Answer only with the summary. Avoid generic or overly broad answers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussione parametro \"fwd_hooks\" del metodo \"run_with_hooks_with_saes\".\n",
    "Riceve coppie \"nome hook point\"/hook_function. Il nome dell'hookpoint è legato al nome del layer del modello su cui è stato addestrato il sae. Nel notebook originale veniva estratto da \"sae.cfg.hook_name\", tuttavia sae.cfg nel nostro caso non ha un campo \"hook_name\".\n",
    "Il dizionario di configurazione di un sae ha dei campi predefiniti che dipendono dall' \"architecture\" con cui si sceglie di ricostruire il sae. Nel nostro caso l'architecture è \"standard\", ma questa non contempla il campo \"hook_name\" nel dizionario di configurazione, sebbene quello estratto da huggingface, che lo usiamo per costruire il sae, lo abbia.\n",
    "Per questo motivo il nome dell'hookpoint viene estratto dal \"cfg_dict\" di partenza scaricato da huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:11:13.595438Z",
     "iopub.status.busy": "2025-10-27T12:11:13.594730Z",
     "iopub.status.idle": "2025-10-27T12:11:13.602721Z",
     "shell.execute_reply": "2025-10-27T12:11:13.602013Z",
     "shell.execute_reply.started": "2025-10-27T12:11:13.595412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_feature_act_hook(act, hook, feature, value):\n",
    "    act[:,:,feature] = value\n",
    "\n",
    "def get_intervention_tokens(model: HookedSAETransformer, pile, f: Feature, value=200, sae=None):\n",
    "\n",
    "    # print(f\"Memoria GPU iniziale: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "\n",
    "    prompts = pile[torch.randint(0, len(pile), (32,))].to('cuda:0') \n",
    "\n",
    "    clean_logits = model.run_with_saes(prompts, saes=[sae])\n",
    "    \n",
    "    pos_inter_logits = model.run_with_hooks_with_saes(prompts, saes=[sae], fwd_hooks=[(f\"{sae.cfg.metadata.hook_name}.hook_sae_acts_post\", \n",
    "                                                                                   functools.partial(set_feature_act_hook, feature=f.feature, value=value))])\n",
    "    neg_inter_logits = model.run_with_hooks_with_saes(prompts, saes=[sae], fwd_hooks=[(f\"{sae.cfg.metadata.hook_name}.hook_sae_acts_post\", \n",
    "                                                                                   functools.partial(set_feature_act_hook, feature=f.feature, value=-value))])\n",
    "\n",
    "    # print(f\"Memoria GPU Step 1: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "    \n",
    "    del prompts\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # print(f\"Memoria GPU Step 2: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "    \n",
    "    # Calcolo la differenza media nei logits rispetto al modello \"pulito\"\n",
    "    pos_diff_logits = (pos_inter_logits - clean_logits).mean(dim=0).mean(dim=0)\n",
    "    neg_diff_logits = (neg_inter_logits - clean_logits).mean(dim=0).mean(dim=0)\n",
    "\n",
    "    del clean_logits, pos_inter_logits, neg_inter_logits\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # print(f\"Memoria GPU Step 3: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "\n",
    "    # token più sensibili positivamente con setting del valore positivo e negativo rispettivamente\n",
    "    pos_toks = model.to_str_tokens(pos_diff_logits.topk(10).indices) + model.to_str_tokens(neg_diff_logits.topk(10, largest=False).indices)\n",
    "\n",
    "    # token più sensibili negativamente con setting del valore positivo e negativo rispettivamente\n",
    "    neg_toks = model.to_str_tokens(pos_diff_logits.topk(10, largest=False).indices) + model.to_str_tokens(neg_diff_logits.topk(10).indices)\n",
    "\n",
    "    del pos_diff_logits, neg_diff_logits\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # print(f\"Memoria GPU Step finale: {torch.cuda.memory_allocated()/1e9:.2f}GB\")\n",
    "    \n",
    "    return pos_toks + neg_toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:11:16.170317Z",
     "iopub.status.busy": "2025-10-27T12:11:16.169790Z",
     "iopub.status.idle": "2025-10-27T12:11:43.800667Z",
     "shell.execute_reply": "2025-10-27T12:11:43.799697Z",
     "shell.execute_reply.started": "2025-10-27T12:11:16.170293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pile = m.get_pile_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:11:51.453528Z",
     "iopub.status.busy": "2025-10-27T12:11:51.453184Z",
     "iopub.status.idle": "2025-10-27T12:11:51.458471Z",
     "shell.execute_reply": "2025-10-27T12:11:51.457690Z",
     "shell.execute_reply.started": "2025-10-27T12:11:51.453500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_TokenChange_explanation(f: Feature):\n",
    "    tokens = get_intervention_tokens(m.m, pile, f, value=10, sae=sae)\n",
    "    \n",
    "    token_change_user_prompt = str(list(set(tokens)))\n",
    "    token_change_exp = get_description(TOKEN_CHANGE_SYS_PROMPT, token_change_user_prompt)\n",
    "    \n",
    "    del tokens, token_change_user_prompt\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return token_change_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Input score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo metodo valuta **la qualità della spiegazione di una feature**, sulla base degli **input che attivano la feature stessa**.  \n",
    "Data una feature f, si fornisce ad un LLM ausiliario la sua spiegazione, gli si chiede di generare due insiemi di k esempi:\n",
    "- **attivanti** (che dovrebbero attivare la feature),\n",
    "- **neutri** (che non dovrebbero attivarla).\n",
    "\n",
    "Ogni esempio viene passato attraverso il modello da spiegare per ottenere l’attivazione della feature.\n",
    "L'attivazione di una feature per un esempio, viene calcolata come il massimo dell’attivazione su tutte le token-position. \n",
    "\n",
    "Il test ha esito positivo se l'attivazione media sui k esempi attivanti, è maggiore dell'attivazione media sui k esempi non attivanti. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:11:54.092796Z",
     "iopub.status.busy": "2025-10-27T12:11:54.092190Z",
     "iopub.status.idle": "2025-10-27T12:11:54.096675Z",
     "shell.execute_reply": "2025-10-27T12:11:54.096056Z",
     "shell.execute_reply.started": "2025-10-27T12:11:54.092771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#prompt\n",
    "GEN_LISTS_PROMPT = \"I'm going to give you explanations and interpretations of features from LLMs. You must take in each expalantion, and generate 5 sentences for which you think the feature will have a high activation, and 5 for which they'll have a low activation. For the high activation, make sure to choose ones that will cause a high activation with high confidence - you don't have to include all groups, just make examples that you're confident will have high activation. Make the sentences both include the words from the explanation, and represent the concept. Try to use specific examples, and make them literal interpretations of the explanation, without trying to generalize. Low activation sentences should have nothing to do with the interpretation - i.e. they should by orthogonal and completely unrelated. Please output the response in json format with a 'positive' key and a 'negative' key. Output only the json and no other explanation. Make sure the json is formatted correctly - do not include any '`' backtick characters characters, i.e. do not format as code, just return the json text. The explanations should be five and five overall, not per line.\\n\\n{explanation}\\n\"\n",
    "\n",
    "FIX_JSON_PROMPT = \"\"\"Please fix this json that is not formatted correctly. Write only the fixed json. \n",
    "DO NOT write anything but the json. No comments. Only the json itself.\\n\\n{json}\\n\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:11:55.464134Z",
     "iopub.status.busy": "2025-10-27T12:11:55.463614Z",
     "iopub.status.idle": "2025-10-27T12:11:55.474538Z",
     "shell.execute_reply": "2025-10-27T12:11:55.473903Z",
     "shell.execute_reply.started": "2025-10-27T12:11:55.464107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_lists(explanation):\n",
    "    \"\"\"\n",
    "    Generates 5 positive and negative activating examples based on our explanation\n",
    "    \"\"\"\n",
    "    return explainer([{\"role\": \"user\", \"content\": GEN_LISTS_PROMPT.format(explanation=explanation)}])\n",
    "    \n",
    "\n",
    "#parsing dell'output generato dal modello\n",
    "def get_pos_neg(lists):\n",
    "    try:\n",
    "        json_content = lists.strip().strip(\"json\").strip('`').removeprefix('json\\n').removesuffix('\\n').strip().strip(\"json\").strip(\"`\").strip('`').removeprefix('json\\n').removesuffix('\\n')\n",
    "        j = json.loads(json_content)\n",
    "        return j[\"positive\"], j[\"negative\"]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for top in range(3):\n",
    "        for bottom in range(2):\n",
    "            bottom = None if bottom == 0 else -bottom\n",
    "            new_lists = lists.splitlines()[top:bottom]\n",
    "\n",
    "            try:\n",
    "                j = json.loads(new_lists)\n",
    "                return j[\"positive\"], j[\"negative\"]\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    lists = explainer([{\"role\": \"user\", \"content\": FIX_JSON_PROMPT.format(json=lists)}])\n",
    "    try:\n",
    "        json_content = lists.strip().strip(\"json\").strip('`').removeprefix('json\\n').removesuffix('\\n').strip().strip(\"json\").strip(\"`\").strip('`').removeprefix('json\\n').removesuffix('\\n')\n",
    "        j = json.loads(json_content)\n",
    "        return j[\"positive\"], j[\"negative\"]\n",
    "    except:\n",
    "        logger.error(lists)\n",
    "        raise\n",
    "\n",
    "#calcolo delle attivazioni\n",
    "def get_pos_neg_acts(model: HookedSAETransformer, pos, neg, f: Feature, pre_relu=False, sae=None):\n",
    "\n",
    "    pos_cache = model.run_with_cache_with_saes(pos, saes=[sae], return_type=None)[1]\n",
    "    neg_cache = model.run_with_cache_with_saes(neg, saes=[sae], return_type=None)[1]\n",
    "    \n",
    "    relu = \"pre\" if pre_relu else \"post\"\n",
    "\n",
    "    # we take the maximal activation of the feature across all sentences across all tokens\n",
    "    # we prefer that because we can generate multiple exps per feature and then take the best activations across all exps\n",
    "    if sae is None:  # transluce\n",
    "        block = f\"blocks.{f.layer}.mlp.hook_post\"\n",
    "    else:\n",
    "        block = f\"{sae.cfg.metadata.hook_name}.hook_sae_acts_{relu}\"\n",
    "        \n",
    "    pos_act_max_all = pos_cache[block][:, :, f.feature].max().item()   #massimo assoluto sui positivi\n",
    "    neg_act_max_all = neg_cache[block][:, :, f.feature].max().item()   #massimo assoluto sui negativi\n",
    "    pos_act_max_toks = pos_cache[block][:, :, f.feature].max(dim=-1).values.mean().item()     #attivazione media sui positivi\n",
    "    neg_act_max_toks = neg_cache[block][:, :, f.feature].max(dim=-1).values.mean().item()     #attivazione media sui negativi\n",
    "\n",
    "    return pos_act_max_all, neg_act_max_all, pos_act_max_toks, neg_act_max_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:11:57.979600Z",
     "iopub.status.busy": "2025-10-27T12:11:57.978866Z",
     "iopub.status.idle": "2025-10-27T12:11:57.986464Z",
     "shell.execute_reply": "2025-10-27T12:11:57.985694Z",
     "shell.execute_reply.started": "2025-10-27T12:11:57.979572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class InputScore:\n",
    "    pos_act_all: float\n",
    "    neg_act_all: float\n",
    "    pos_act_toks: float\n",
    "    neg_act_toks: float\n",
    "    pos_list: float\n",
    "    neg_list: float\n",
    "    \n",
    "    def success(self) -> bool:\n",
    "        # the input metric final test for success\n",
    "        return self.pos_act_toks > self.neg_act_toks\n",
    "    \n",
    "    @classmethod\n",
    "    def from_row(cls, row):\n",
    "        return cls(**vars(row))\n",
    "    \n",
    "\n",
    "def get_input_score(description: str, m: Model, f: Feature, sae=None, verbose=False) -> InputScore:\n",
    "    lists = get_lists(description)   # generare le frasi di test   \n",
    "    # print (lists)\n",
    "    pos_list, neg_list = get_pos_neg(lists)   # estrarre le frasi di test\n",
    "    if verbose:\n",
    "        print (\"====Calcolo Input Success====\")\n",
    "        print(\"=====Frasi positive=====\")\n",
    "        print(pos_list)\n",
    "        print(\"=====Frasi negative=====\")\n",
    "        print(neg_list)\n",
    "    pos_act_all, neg_act_all, pos_act_toks, neg_act_toks = get_pos_neg_acts(m.m, pos_list, neg_list, f, sae=sae)\n",
    "    if verbose:\n",
    "        print(\"attivazione massima frasi postive: \", pos_act_all)\n",
    "        print(\"attivazione massima frasi negative:\" , neg_act_all)\n",
    "        \n",
    "        print(\"attivazione media frasi postive: \", pos_act_toks)\n",
    "        print(\"attivazione media frasi negative:\" , neg_act_toks)\n",
    "    return InputScore(pos_act_all, neg_act_all, pos_act_toks, neg_act_toks, pos_list, neg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Output score (steering success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo metodo valuta quanto la **spiegazione di una feature rifletta l’influenza della feature sugli output** del modello. \n",
    "\n",
    "Si generano tre set di testi a partire da prompt neutri:  \n",
    "- il primo set di testi viene generato, **amplificando la feature in esame**,  \n",
    "- altri due set di testi vengono generati, amplificando due feature campionate in maniera random.\n",
    "\n",
    "Amplificare una feature, significa imporre un'elevata attivazione per quella feature. Il valore di attivazione viene calcolato in modo tale da **garantire un determinato valore di KL-divergence** tra i logit steered e non-steered.\n",
    "\n",
    "\n",
    "Fissato un determinato valore di KL-divergence (ad esempio 0.1), si fa una ricerca binaria in un intervallo tra 1 a 1000, per individuare l'attivazione per quella feature che determina quel valore di KL-divergence.\n",
    "\n",
    "I tre set di testi vengono passati ad un **LLM giudice**, che ha il compito di identificare quale set è stato più verosimilmente generato per amplificazione della feature in esame, sulla base della spiegazione fornita.\n",
    "\n",
    "Il test ha successo se l'LLM giudice individua il corretto set di token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:12:01.032318Z",
     "iopub.status.busy": "2025-10-27T12:12:01.031585Z",
     "iopub.status.idle": "2025-10-27T12:12:01.036084Z",
     "shell.execute_reply": "2025-10-27T12:12:01.035220Z",
     "shell.execute_reply.started": "2025-10-27T12:12:01.032291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NUM_OF_DISTS=2 # numero di distrattori nell’output score\n",
    "KL_DIV_VALUES = [0.1, 0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:12:02.178355Z",
     "iopub.status.busy": "2025-10-27T12:12:02.177682Z",
     "iopub.status.idle": "2025-10-27T12:12:02.182970Z",
     "shell.execute_reply": "2025-10-27T12:12:02.182410Z",
     "shell.execute_reply.started": "2025-10-27T12:12:02.178326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_METRIC_GENERATION_PROMPTS = [\"The explanation is simple:\", \"I think\", \"We\"]\n",
    "\n",
    "OUTPUT_METRIC_SYS_PROMPT = \"\"\"You are analyzing the behavior of a specific neuron in a language model. You will receive:\n",
    "\n",
    "1. A hypothesized explanation for what concept the neuron represents (e.g., specific tokens, themes, or ideas).\n",
    "2. Three sets of completions, one generated by amplifying the activation of the neuron in question, and one of a random neuron across the same prompts.\n",
    "\n",
    "Your goal is to identify which set of completions is more likely the result of amplifying the neuron in question. To do this:\n",
    "- Look for completions where the **literal words** or the **ideas/themes** described in the explanation occur more frequently or with greater emphasis.\n",
    "- Remember that amplification may highlight specific words or their broader contextual meanings, meaning that a lot of the times they might be very noisy, but contain keywords that appear in the explanation.\n",
    "- Your answer should be based on the **content** of the completions, not the quality of the language model's output.\n",
    "- Your reasoning should be sound, don't make overly elaborate and far-fetched connections.\n",
    "\n",
    "The first line in your response should be a brief explanation of your choice - what made you choose that set of completions.\n",
    "The second line must be only the set number you think matches the description (i.e., 1, 2 or 3) and no other text. You must pick one of the three sets.\n",
    "\"\"\"\n",
    "\n",
    "STEERING_AMP_PROMPT_TEMPLATE = \"\"\"<{amplification}> {completions}\\n\"\"\"\n",
    "\n",
    "STEERING_FULL_PROMPT_TEMPLATE = \"\"\"Explanation: {explanation}\n",
    "\n",
    "# Set 1\n",
    "{amplifications1}\n",
    "\n",
    "# Set 2\n",
    "{amplifications2}\n",
    "\n",
    "# Set 3\n",
    "{amplifications3}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:12:03.958149Z",
     "iopub.status.busy": "2025-10-27T12:12:03.957414Z",
     "iopub.status.idle": "2025-10-27T12:12:03.967337Z",
     "shell.execute_reply": "2025-10-27T12:12:03.966596Z",
     "shell.execute_reply.started": "2025-10-27T12:12:03.958122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_feature_act_kl_hook(act, hook, feature: int, value):\n",
    "    \"\"\"\n",
    "    callback che viene richiamata quando il modello calcola le attivazioni\n",
    "    \n",
    "    - act è il tensore della attivazioni, tipicamente (batch_size, seq_len, d_model)\n",
    "    - feature è l'indice della feature\n",
    "    - value è il valore di attivazione della feature\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    act[:,:,feature] = value\n",
    "\n",
    "def kl_div(p, q, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Get the KL Divergence between p and q\n",
    "    \"\"\"\n",
    "    p = p.clamp(min=eps)  #fa clampimg di un epsilon per evitare di fare log(0)\n",
    "    q = q.clamp(min=eps)  #fa clamping di un epsilon per evitare di fare log(0)\n",
    "    return torch.sum(p * (torch.log(p) - torch.log(q)), dim=-1)\n",
    "    #in generale p e q sono dei tensori di forma (batch_size, seq_len, vocab_size), quindi bisogna fare le operazioni sull'ultima dimensione\n",
    "\n",
    "#restituisce la divergenza di kullback leibler tra le attivazioni pulite e le attivazioni dopo il clamping della feature. \n",
    "def get_kl_div(model: Model, prompts, f: Feature, value, sae=None):\n",
    "    \"\"\"\n",
    "    Get KL Divergence between clean and hooked activations when clamping the feature to value, \n",
    "    averaged over all of the prompts\n",
    "    \"\"\"\n",
    "    #tokenizzazione\n",
    "    toks = model.m.to_tokens(prompts)\n",
    "\n",
    "    clean_probs = model.m.run_with_saes(toks, saes=[sae])\n",
    "    hooked_probs = model.m.run_with_hooks_with_saes(toks, saes=[sae], fwd_hooks=[(f\"{sae.cfg.metadata.hook_name}.hook_sae_acts_post\", \n",
    "                                                                              functools.partial(set_feature_act_kl_hook, feature=f.feature, value=value))]).softmax(dim=-1)\n",
    "    clean_probs = clean_probs.softmax(dim=-1)\n",
    "\n",
    "    # Remove logits which are padding tokens\n",
    "    clean_probs[toks == 0] = 0\n",
    "    hooked_probs[toks == 0] = 0\n",
    "\n",
    "    # return hooked_probs, clean_probs\n",
    "    kl = kl_div(clean_probs, hooked_probs)\n",
    "    means = []\n",
    "    for row in kl:\n",
    "        means.append(row[row != 0].mean().item())\n",
    "\n",
    "    return np.mean(means)\n",
    "    #Alla fine si restituisce la media delle medie delle divergenze di kullback-leibler\n",
    "\n",
    "\n",
    "#trova il valore di attivazione per una feature ad una definita soglia di divergenza di kullback-leibler.\n",
    "#fa una ricerca binaria all'interno di un certo intervallo (0, 1000)\n",
    "def get_activation_for_kl(model: Model, prompts, f: Feature, target_kl, high_thresh=0.1, neg=False, verbose=False, sae=None):\n",
    "    \"\"\"\n",
    "    Find the activation value we need for the desired target KL Divergence value\n",
    "    \"\"\"\n",
    "    # Do binary search between 0 and 1000\n",
    "    low, high = (-1000, -1) if neg else (1, 1000)\n",
    "    kl = -1\n",
    "    mid = 0\n",
    "    while (low+1 < high) and (kl < target_kl or kl > target_kl + high_thresh):\n",
    "        mid = (low + high) // 2\n",
    "        kl = get_kl_div(model, prompts, f, mid, sae=sae)\n",
    "\n",
    "        if (neg and kl < target_kl) or (not neg and kl > target_kl):\n",
    "            high = mid\n",
    "        else:\n",
    "            low = mid\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Low: {low}, High: {high}, Mid: {mid}, KL: {kl}, Target KL: {target_kl}\")\n",
    "\n",
    "    return mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:12:06.463222Z",
     "iopub.status.busy": "2025-10-27T12:12:06.462924Z",
     "iopub.status.idle": "2025-10-27T12:12:06.475289Z",
     "shell.execute_reply": "2025-10-27T12:12:06.474701Z",
     "shell.execute_reply.started": "2025-10-27T12:12:06.463199Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_completions_for_kl_val(model: Model, prompts, f: Feature, kl, neg=False, sae=None):\n",
    "    act = get_activation_for_kl(model, prompts, f, kl, neg=neg, sae=sae)\n",
    "    positive = hooked_gen(model.m.to_tokens(prompts), model, f, n=25, value=act, temperature=0.75, sae=sae)\n",
    "    return [x.replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\\\\r\") for x in positive]\n",
    "\n",
    "@dataclass\n",
    "class OutputScore:\n",
    "    correct_choice: int\n",
    "    chosen_index: int\n",
    "\n",
    "    def success(self):\n",
    "        if self.chosen_index==4:\n",
    "            return \"failed test\"\n",
    "        # the output metric final test for success\n",
    "        return self.correct_choice == self.chosen_index\n",
    "    \n",
    "def to_int(s):\n",
    "    try:\n",
    "        return int(re.search(r'\\d+', s).group())\n",
    "    except: \n",
    "        return 4\n",
    "\n",
    "#Per ogni valore di divergenza di kullback-leibler fissato, per ognuno dei tre prompt neutrali genera gli output, facendo steering di due featur casuali.\n",
    "def get_random_amps(m: Model, f: Feature, sae=sae):\n",
    "    random_amps = [\"\" for _ in range(NUM_OF_DISTS)]\n",
    "    for i in KL_DIV_VALUES:\n",
    "        for x in range(NUM_OF_DISTS):\n",
    "            ampy = random_amps[x]\n",
    "            \n",
    "            random_feature = random.sample(range(sae.cfg.d_sae), 1)[0]\n",
    "    \n",
    "            random_f = Feature(m.model_id, random_feature, f.layer, f.type)\n",
    "            rand_pos = get_completions_for_kl_val(m, OUTPUT_METRIC_GENERATION_PROMPTS, random_f, i, sae=sae)\n",
    "            ampy += f\"\\n<{'+' if i >= 0 else ''}{i}>\" + f\"\\n<{'+' if i >= 0 else ''}{i}>\".join([f\"'{OUTPUT_METRIC_GENERATION_PROMPTS[x]}': '{rand_pos[x]}'\" for x in range(len(rand_pos))])\n",
    "            random_amps[x] = ampy\n",
    "\n",
    "    return random_amps\n",
    "\n",
    "\n",
    "def get_output_score(description: str, m: Model, f: Feature, sae=None, verbose=False) -> OutputScore:\n",
    "    \n",
    "    amps = \"\"\n",
    "    for i in KL_DIV_VALUES:\n",
    "        pos = get_completions_for_kl_val(m, OUTPUT_METRIC_GENERATION_PROMPTS, f, abs(i), neg=False if i>=0 else True, sae=sae)\n",
    "        amps += f\"\\n<{'+' if i >= 0 else ''}{i}>\" + f\"\\n<{'+' if i >= 0 else ''}{i}>\".join([f\"'{OUTPUT_METRIC_GENERATION_PROMPTS[x]}': '{pos[x]}'\" for x in range(len(pos))])\n",
    "\n",
    "\n",
    "\n",
    "    random_amps = get_random_amps(m, f)\n",
    "    random_amps = random_amps.copy()\n",
    "    random_amps.append(amps)\n",
    "\n",
    "    random_indices = random.sample(range(3), 3)\n",
    "    ordered_amps = [random_amps[i] for i in random_indices]\n",
    "    correct_choice = random_indices.index(2) + 1\n",
    "    \n",
    "    prompt = STEERING_FULL_PROMPT_TEMPLATE.format(explanation=description, amplifications1=ordered_amps[0], amplifications2=ordered_amps[1], amplifications3=ordered_amps[2])\n",
    "    \n",
    "    if (verbose):\n",
    "        print (\"====Calcolo steering success====\")\n",
    "        print(\"PROMPT EXPLAINER: \", prompt)\n",
    "    \n",
    "    response = explainer([\n",
    "        {\"role\": \"user\", \"content\": OUTPUT_METRIC_SYS_PROMPT + \"\\n\\n\" + prompt}\n",
    "    ], weak=True)\n",
    "\n",
    "    \n",
    "\n",
    "    chosen_index = to_int(response.splitlines()[-1])\n",
    "    \n",
    "\n",
    "    if (verbose):\n",
    "        print (\"Chosen index: \", chosen_index)\n",
    "        print (\"Correct choice: \", correct_choice)\n",
    "    return OutputScore(correct_choice, chosen_index)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:12:09.067083Z",
     "iopub.status.busy": "2025-10-27T12:12:09.066415Z",
     "iopub.status.idle": "2025-10-27T12:12:09.073722Z",
     "shell.execute_reply": "2025-10-27T12:12:09.072948Z",
     "shell.execute_reply.started": "2025-10-27T12:12:09.067057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def gen_hook(clean_act, hook,  feature: int, value, sae=None):\n",
    "    \"\"\"\n",
    "    Manually steers the value inside an SAE activation using the basic activation\n",
    "    :param clean_act: the basic activation before the SAE\n",
    "    \"\"\"\n",
    "    if sae is None:  # transluce\n",
    "        clean_act[:,:,feature] = value\n",
    "        return clean_act\n",
    "    \n",
    "    encoded_act = sae.encode(clean_act)\n",
    "    dirty_act = sae.decode(encoded_act)\n",
    "    error_term = clean_act - dirty_act\n",
    "\n",
    "    encoded_act[:,:,feature] = value\n",
    "    hooked_act = sae.decode(encoded_act) + error_term\n",
    "\n",
    "    return hooked_act\n",
    "    \n",
    "#generazione del modello di n tokens con lo steering effettuato    \n",
    "def hooked_gen(prompt, model: Model, f: Feature, n=25, value=10, do_sample= True, verbose=False, temperature=1, sae=None):\n",
    "    \"\"\"\n",
    "    :param prompt: the prompt for which the model will generate text\n",
    "    :param value: the value to amplify the feature with\n",
    "    \"\"\"\n",
    "    model.m.reset_hooks()\n",
    "    block = f\"blocks.{f.layer}.mlp.hook_pre\" if sae is None else cfg_dict['metadata']['hook_name']\n",
    "    model.m.add_hook(block, functools.partial(gen_hook, feature=f.feature, value=value, sae=sae))\n",
    "    output = model.m.generate(prompt, max_new_tokens=n, do_sample=do_sample, verbose=verbose, temperature=temperature)\n",
    "    model.m.reset_hooks()\n",
    "\n",
    "    return [x[len(model.m.to_string(prompt[i])):] for i, x in enumerate(model.m.to_string(output))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features analisys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:12:11.737685Z",
     "iopub.status.busy": "2025-10-27T12:12:11.737409Z",
     "iopub.status.idle": "2025-10-27T12:12:11.742553Z",
     "shell.execute_reply": "2025-10-27T12:12:11.741672Z",
     "shell.execute_reply.started": "2025-10-27T12:12:11.737664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def explain_latents(f: Feature):\n",
    "    explanation_MaxAct, surprisal_score = get_MaxAct_explanation(f)\n",
    "    explanation_VocabProj = get_VocabProj_explanation(f)\n",
    "    explanation_TokenChange = get_TokenChange_explanation(f)\n",
    "    return explanation_MaxAct, surprisal_score, explanation_VocabProj, explanation_TokenChange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:12:13.421677Z",
     "iopub.status.busy": "2025-10-27T12:12:13.421392Z",
     "iopub.status.idle": "2025-10-27T12:12:13.427323Z",
     "shell.execute_reply": "2025-10-27T12:12:13.426643Z",
     "shell.execute_reply.started": "2025-10-27T12:12:13.421653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_latents(sample_dim, seed=seed):\n",
    "    random.seed(seed)  # Imposta il seed per la riproducibilità\n",
    "    latents_to_explain = []\n",
    "\n",
    "    def condition(num):\n",
    "        f = Feature(model_name, num, num_layer , type_layer, [], size=\"131k\") \n",
    "        raw_dir = f\"/kaggle/working/progetto_nlp_2/results/test-run/latents/{hookpoint_dir}\"\n",
    "        hookpoints, latents = inizialize_environment(f.feature-1, f.feature+2, raw_dir)\n",
    "        buffers = create_buffer(hookpoints, latents, raw_dir)\n",
    "        tokens = upload_dataset()\n",
    "        records = extract_data(buffers, tokens)\n",
    "        convert_feature(f, records)\n",
    "        return len(f.activations) > 1\n",
    "\n",
    "    while len(latents_to_explain) < sample_dim:\n",
    "        num = random.randint(1, cfg_dict[\"d_sae\"])  #controllare\n",
    "        if condition(num):\n",
    "            latents_to_explain.append(num)\n",
    "\n",
    "    return latents_to_explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:12:15.528731Z",
     "iopub.status.busy": "2025-10-27T12:12:15.528041Z",
     "iopub.status.idle": "2025-10-27T12:12:15.533779Z",
     "shell.execute_reply": "2025-10-27T12:12:15.533009Z",
     "shell.execute_reply.started": "2025-10-27T12:12:15.528705Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calcolate_scores(techniques, explanations, f: Feature, verbose=False):\n",
    "    result = {}\n",
    "    assert len(explanations) == 3\n",
    "    for idx, exp in enumerate(explanations):\n",
    "        print(\"Explanation from \",techniques[idx], \" : \", exp)\n",
    "        vp_input_score = get_input_score(exp, m, f, sae=sae, verbose=verbose)\n",
    "        vp_out_score = get_output_score(exp, m, f, sae=sae, verbose=verbose)\n",
    "        result[techniques[idx]] = [vp_input_score.success(), vp_out_score.success()]\n",
    "        print(\" \\n Success for InputScore: \", vp_input_score.success(), \" \\n Success for OutputScore: \", vp_out_score.success())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T12:12:19.765237Z",
     "iopub.status.busy": "2025-10-27T12:12:19.764588Z",
     "iopub.status.idle": "2025-10-27T12:12:19.826006Z",
     "shell.execute_reply": "2025-10-27T12:12:19.825019Z",
     "shell.execute_reply.started": "2025-10-27T12:12:19.765201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "range_latents = generate_latents(sample_dim)\n",
    "all_results = []\n",
    "\n",
    "for latent in range_latents:\n",
    "    f = Feature(model_name, latent, num_layer, type_layer, [], size=\"16k\") \n",
    "    print(f\"=== Explaining latent {latent} ===\")\n",
    "\n",
    "    explanations = [exp for exp in explain_latents(f)]\n",
    "    surprisal_score = explanations[1]\n",
    "    explanations = explanations[:1] + explanations[2:]\n",
    "    scores = calcolate_scores([\"MaxAct\", \"VocabProj\", \"TokenChange\"], explanations, f, True)\n",
    "\n",
    "    latent_result = {\"latent\": latent, \"surprisal_score\": surprisal_score, \"techniques\": []}\n",
    "\n",
    "    for idx, technique_name in enumerate(scores):\n",
    "        values = scores[technique_name]\n",
    "        technique_data = {\n",
    "            \"technique\": technique_name,\n",
    "            \"explanation\": explanations[idx],\n",
    "            \"input score\": values[0],\n",
    "            \"output score\": values[1]\n",
    "        }\n",
    "        latent_result[\"techniques\"].append(technique_data)\n",
    "\n",
    "    all_results.append(latent_result)\n",
    "    print(\"==================================\")\n",
    "    with open('latent_explanations.json', 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "\n",
    "with open('latent_explanations.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "print(\"Risultati salvati in latent_explanations.json\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
